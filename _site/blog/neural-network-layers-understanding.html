<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="google-site-verification" content="aexITLS38FdIRzwj25OVWxm87rpa9l-UV0URTyC9cTs" />
  <title>
    <!--深度学习：Neural Network Layers Understanding
    -->
  </title>
  <link rel="stylesheet" href="../css/site.css">
  <link href="//cdn.bootcss.com/font-awesome/4.4.0/css/font-awesome.css" rel="stylesheet">
  <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon" />
  
  <link href="/blog/feed.xml" type="application/rss+xml" rel="alternate" title="Latest 10 blog posts (atom)" />

  

  

  
</head>

<body>
  <div class="header-container">
    <header class="inner">
      <nav>
        <a class="" href="/">关于</a>
        <!--<a class="" href="/cn/project">项目</a>-->
        <!--<a href="/YongYuan_resume.pdf">简历</a>-->
        <a class="" href="/blog/">博客</a>
        <!--<a href="/books/">书单</a>
		<a href="/timeline/">时间线</a>-->
        <a href="https://github.com/str818">Github</a>
      </nav>
      <div class="pull-right right logo">
        <!--<div class="name">
          <a href="/">Yong Yuan</a><br />
          <small>
            <em>
              
                <a href="/">developer</a>
              
            </em>
          </small>
        </div>-->
        <a href="/"><img class="avatar" src="/images/str818.jpg" alt="My profile picture" /></a>
      </div>
      <div class="clear"></div>
    </header>
    <div class="clear"></div>
  </div>

  <link rel="stylesheet" href="../css/blog.css">



<div id="container">
<article>
<div class="inner">
<div class = "post-header">
  <h1>
    深度学习：Neural Network Layers Understanding
  </h1>
  <ul class="meta">
  	<i class="fa fa-calendar"></i>
  	2017年08月29日
  	<i class="space"></i>
  	<i class="fa fa-archive"></i>
  	<a class="tag" href="http://localhost:5001/blog/categories.html#Deep Learning">Deep Learning</a>
  	<i class="space"></i>
  	<i class="fa fa-tags"></i>
  	
  	    <a class="tag" href="http://localhost:5001/tag/#DL">DL</a>
  	    <i class="space"></i>
  	
  	<i class="space"></i> 字数:5007
  </ul>
</div>
</div>

  <div class="post">
        <div id="content">
          <div class ="sidebox">
              <div class ="sidebar">
              <div class="toc"></div>
              </div>
          </div>
  	   <blockquote>
  <p>我想做又应该做的事，都会做到；我想做却不应做的事，都会戒掉。</p>
</blockquote>

<h2 id="inner-product-layer">Inner Product Layer</h2>

<p>Inner Product Layer即全连接层，对于IP层的理解，可以简单的将其视为矩阵<code class="highlighter-rouge">1*N</code>和矩阵<code class="highlighter-rouge">N*M</code>相乘后得到<code class="highlighter-rouge">1*M</code>的维度向量。</p>

<p>举个简单的例子，比如输入全连接层的是一个<code class="highlighter-rouge">3*56*56</code>维度的数据，假设未知的权重维度为<code class="highlighter-rouge">N*M</code>，假设全连接层的输出为<code class="highlighter-rouge">num_ouput = 4096</code>，为了计算全连接层的输出，全连接层会将输入的数据<code class="highlighter-rouge">3*56*56</code> reshape 成为<code class="highlighter-rouge">1*N</code>的形式，即<code class="highlighter-rouge">1x(56x56x3) = 1x9408</code>，所以：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N = 9408
M = num_ouput = 4096
</code></pre></div></div>

<p>由此，我们做了一个<code class="highlighter-rouge">(1x9408)</code>矩阵和<code class="highlighter-rouge">(9408x4096)</code>矩阵的乘法。如果<code class="highlighter-rouge">num_output</code>的值改变成为100，则做的是一个<code class="highlighter-rouge">(1x9408)</code>矩阵和<code class="highlighter-rouge">(9408x100)</code>矩阵的乘法。<strong>Inner Product layer（常被称为全连接层）将输入视为一个vector，输出也是一个vector（height和width被设为1）</strong>。下面是IP层的示意图</p>

<p><img src="http://ose5hybez.bkt.clouddn.com/2017/draft/fcgemm_corrected.png" alt="" /></p>

<blockquote>
  <p>图片摘自<a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/">Why GEMM is at the heart of deep learning</a></p>
</blockquote>

<p>增大<code class="highlighter-rouge">num_output</code>会使得模型需要学习的权重参数增加。IP层一个典型的例子：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
</code></pre></div></div>

<p>有了上面对IP层的理解，对<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/inner_product_layer.cpp">caffe inner_product_layer.cpp</a>中Forward的理解就比较自然了。下面是Caffe的IP层在CPU上的实现：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">InnerProductLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">weight</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">blobs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">caffe_cpu_gemm</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">transpose_</span> <span class="o">?</span> <span class="n">CblasNoTrans</span> <span class="o">:</span> <span class="n">CblasTrans</span><span class="p">,</span>
      <span class="n">M_</span><span class="p">,</span> <span class="n">N_</span><span class="p">,</span> <span class="n">K_</span><span class="p">,</span> <span class="p">(</span><span class="n">Dtype</span><span class="p">)</span><span class="mf">1.</span><span class="p">,</span>
      <span class="n">bottom_data</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">Dtype</span><span class="p">)</span><span class="mf">0.</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">bias_term_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">caffe_cpu_gemm</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">M_</span><span class="p">,</span> <span class="n">N_</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">Dtype</span><span class="p">)</span><span class="mf">1.</span><span class="p">,</span>
        <span class="n">bias_multiplier_</span><span class="p">.</span><span class="n">cpu_data</span><span class="p">(),</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">blobs_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span> <span class="p">(</span><span class="n">Dtype</span><span class="p">)</span><span class="mf">1.</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>上面完成矩阵与矩阵相乘的函数是<code class="highlighter-rouge">caffe_cpu_gemm&lt;Dtype&gt;</code>（见<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp">math_functions.cpp</a>)，<code class="highlighter-rouge">caffe_cpu_gemm</code>函数矩阵相乘的具体数学表示形式为：</p>

<script type="math/tex; mode=display">\begin{equation}
   C=alpha*TransA(A)*TransB(B) + beta*C\\\\
\end{equation}</script>

<p>上式中<code class="highlighter-rouge">TransX</code>是对<code class="highlighter-rouge">X</code>做的一种矩阵变换，比如转置、共轭等，具体是<code class="highlighter-rouge">cblas.h</code>中定义的为枚举类型。在<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp">math_functions.cpp</a>中，除了定义矩阵与矩阵相乘的<code class="highlighter-rouge">caffe_cpu_gemm</code>外，还定义了矩阵与向量的相乘，具体的函数为<code class="highlighter-rouge">caffe_cpu_gemv</code>，其数学表示形式为：</p>

<script type="math/tex; mode=display">\begin{equation}
   C=alpha*TransA(A)*y + beta*y\\\\
\end{equation}</script>

<p>上面表达式中，<code class="highlighter-rouge">y</code>是向量，不是标量。</p>

<h4 id="参考">参考</h4>

<ol>
  <li><a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/">Why GEMM is at the heart of deep learning</a></li>
  <li><a href="https://stackoverflow.com/questions/35788873/what-is-the-output-of-fully-connected-layer-in-cnn">What is the output of fully connected layer in CNN?</a></li>
  <li><a href="http://blog.csdn.net/seven_first/article/details/47378697">caffe_cpu_gemm函数</a></li>
  <li><a href="http://blog.csdn.net/u011762313/article/details/47361571">Caffe学习：Layers</a></li>
  <li><a href="http://caffe.berkeleyvision.org/tutorial/layers.html">Caffe Layers</a></li>
</ol>

<h2 id="gemm">GEMM</h2>

<p>在上面的IP层中，我们已经涉及到了GEMM的知识，在这一小节里面，不妨对该知识点做一个延伸。</p>

<p>GEMM是BLAS (Basic Linear Algebra Subprograms)库的一部分，该库在1979年首次创建。为什么GEMM在深度学习中如此重要呢？我们可以先来看一个图：</p>

<p><img src="http://ose5hybez.bkt.clouddn.com/2017/draft/gemm_cup_gpu.png" alt="" /></p>

<blockquote>
  <p>图片摘自<a href="http://daggerfs.com/">Yangqing Jia</a>的<a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.pdf">thesis</a></p>
</blockquote>

<p>上图是采用AlexNet对CNN网络中不同layer GPU和CPU的时间消耗，从更底层的实现可以看到CNN网络的主要时间消耗用在了FC (for fully-connected)和Conv (for convolution)，而FC和Conv在实现上都将其转为了矩阵相乘的形式。举个例子：</p>

<p><img src="http://ose5hybez.bkt.clouddn.com/2017/draft/cnn_gemm.jpg" alt="" /></p>

<blockquote>
  <p>图片摘自<a href="http://arxiv.org/pdf/1410.0759.pdf">cuDNN: Efficient Primitives for Deep Learning</a></p>
</blockquote>

<p>上面Conv在Caffe中具体实现的时候，会将每一个小的patch拉成一个向量，很多patch构成的向量会构成一个大的矩阵，同样的对于多个卷积核展成一个矩阵形式，从而将图像的卷积转成了矩阵与矩阵的相乘（更形象化的解释参阅<a href="https://www.zhihu.com/question/28385679/answer/44297845">在 Caffe 中如何计算卷积？</a>）。上面可以看到在FC和Conv上消耗的时间GPU占95%，CPU上占89%。因而GEMM的实现高效与否对于整个网络的效率有很大的影响。</p>

<p>那么什么是GEMM呢？GEMM的全称是GEneral Matrix to Matrix Multiplication，正如其字面意思所表达的，GEMM即表示两个输入矩阵进行相乘，得到一个输出的矩阵。两个矩阵在进行相乘的时候，通常会进行百万次的浮点运算。对于一个典型网络中的某一层，比如一个256 row<em>1152 column的矩阵和一个1152 row</em>192 column的矩阵，二者相乘57 million (256 x 1152 x 192)的浮点运算。因而，通常我们看到的情形是，一个网络在处理一帧的时候，需要几十亿的FLOPs（Floating-point operations per second，每秒浮点计算）。</p>

<p>既然知道了GEMM是限制整个网络时间消耗的主要部分，那么我们是不是可以对GEMM做优化调整呢？答案是否定的，GEMM采用Fortran编程语言实现，经过了科学计算编程人员几十年的优化，性能已经极致，所以很难再去进一步的优化，在Nvidia的论文<a href="http://arxiv.org/pdf/1410.0759.pdf">cuDNN: Efficient Primitives for Deep Learning</a>中指出了还存在着其他的一些方法，但是他们最后采用的还是改进的GEMM版本实现。GEMM可匹敌的对手是傅里叶变换，将卷积转为频域的相乘，但是由于在图像的卷积中存在strides，使得傅里叶变换方式很难保持高效。</p>

  	</div>
	</div>
</article>

<!--翻页功能-->
<section class="inner">
<ul class="pager">
    
    <li class="previous">
       <a href="/blog/graduate-after-one-year.html">←知行手记：毕业一周年</a>
    </li>
    
    
    <li class="next">
    	<a href="/blog/fim-fisher-kernel.html">图像检索：Fisher Information Matrix and Fisher Kernel→</a>
    </li>
    
 </ul>
</section>

<!--评论功能-->
<section class="comments inner duoshuo">
<!-- Disqus Comment BEGIN -->
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'yongyuan'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
<!-- Disqus Comment END -->

</section>
</div>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    <!--displayMath: [['$$','$$'], ['\[','\]']],-->
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>



  <!--<div class="separator"></div>-->

  <footer>
    <div  style="border-bottom: 1px solid #eee"></div>
    <!--
	<p class="linkings">
        Friend: Lichao</a>&nbsp&nbsp&nbsp&nbsp<a href="https://yihui.name/cn/">Yihui</a>&nbsp&nbsp&nbsp&nbsp<a href="http://zhangxuezhi.com/">Xuezhi</a>&nbsp&nbsp&nbsp&nbsp<a href="http://www.52ml.net/">52ml</a>&nbsp&nbsp&nbsp&nbsp<a href="http://caoyaqiang.com/">cyq</a>&nbsp&nbsp&nbsp&nbsp<a href="http://personal.ee.surrey.ac.uk/Personal/R.Hu/">Rui Hu</a>&nbsp&nbsp&nbsp&nbsp<a href="http://www.pyimagesearch.com/">pyimagesearch</a>&nbsp&nbsp&nbsp&nbsp<a href="http://bean.logdown.com/">bean</a>&nbsp&nbsp&nbsp&nbsp<a href="http://ml.memect.com/">ml dairy</a>&nbsp&nbsp&nbsp&nbsp<a href="http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html/">Resources</a>
    </p>
	-->
    <!--p class="linkings">
        Utils: <a href="http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html">rogerioferis</a>&nbsp&nbsp&nbsp&nbsp<a href="http://caffe.berkeleyvision.org/">Caffe</a>&nbsp&nbsp&nbsp&nbsp<a href="http://scikit-learn.org/stable/index.html">scikit-learn</a>&nbsp&nbsp&nbsp&nbsp<a href="http://www.astroml.org/index.html">AstroML</a></a>&nbsp&nbsp&nbsp&nbsp<a href="http://scikit-image.org/">scikit-image</a></a>&nbsp&nbsp&nbsp&nbsp<a href="http://gitxiv.com/">GitXiv</a>
    </p-->
    <p>
      Made with <a href="http://jekyllrb.com/">Jekyll</a>,
      hosted on <a href="https://github.com/willard-yuan/willard-yuan.github.io">Github Pages</a>. Inspired by <a href="http://yongyuan.name">Willard</a>.
    </p>
    <!--
	<p>
      <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International</a> 2013-2017
    </p>
	-->
    <!--ul class="links">
      <li>
        <a href="https://github.com/willard-yuan" title="See my code on GitHub">
          <i class="icon-github"></i>
        </a>
      </li>
      <li>
        <a href="https://twitter.com/intent/follow?screen_name=yongyuan001" title="Follow me on Twitter">
          <i class="icon-twitter"></i>
        </a>
      </li>
      <li>
        <a href="https://www.linkedin.com/in/sebastiensaunier" title="Connect with me on LinkedIn">
          <i class="icon-linkedin"></i>
        </a>
      </li>
      <li>
        <a href="https://plus.google.com/+SebastienSaunier?rel=author" title="I need this link for authorship reasons">
          <i class="icon-google-plus"></i>
        </a>
      </li>
      <li>
        <a href="https://pinboard.in/u:ssaunier" title="Browse my bookmarks about Programming">
          <i class="icon-bookmarks"></i>
        </a>
      </li>
      <li>
        <a href="/feed.xml" title="Subscribe to my blog with RSS">
          <i class="icon-feed"></i>
        </a>
      </li>
    </ul-->
  </footer>

<script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

</body>
</html>
