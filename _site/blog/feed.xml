<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>str818's homepage</title>
    <description>Technology Blog</description>
    <link>http://localhost:5001</link>
    <atom:link href="http://localhost:5001/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>专利侵权判定</title>
        <description>&lt;p&gt;发明或者实用新型专利权的保护范围以其权利要求的内容为准，说明书及附图可以用于解释权利要求。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;专利保护范围以权利要求书记载的内容为准，而不是由专利产品确定的&lt;/li&gt;
  &lt;li&gt;在上述前提下，允许利用说明书和附图对权利要求的保护范围作出一定的修正，这种修正是以专利权人对自己的发明创造作出具体说明为依据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一项专利权的权利要求是由一个个具体的技术特征组成的。在一项发明专利中，其权利要求书中至少包含一项独立权利要求，还可以包括从属权利要求。由于独立权利要求是构成一项发明创造必不可少的必要技术特征组成的，
它的保护范围最大，所以&lt;strong&gt;专利权的保护范围首先是由独立权利要求确定的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果缺少了某个技术特征时，会使方案不完整，或者行不通，那该技术特征就是&lt;strong&gt;必要技术特征&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;一般来说，在具体进行专利侵权判定时，应当结合以下几个主要原则加以运用：&lt;/p&gt;

&lt;h4 id=&quot;一-全面覆盖原则&quot;&gt;一 全面覆盖原则&lt;/h4&gt;
&lt;h3 id=&quot;参考&quot;&gt;参考&lt;/h3&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://jekyllcn.com/&quot;&gt;Jekyll官方文档&lt;/a&gt;  &lt;br /&gt;
[2] &lt;a href=&quot;http://blog.csdn.net/itmyhome1990/article/details/50443826&quot;&gt;升级Jekyll 3.0&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/patent-infringement-judgment.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/patent-infringement-judgment.html</guid>
      </item>
    
      <item>
        <title>Windows系统下安装Jekyll</title>
        <description>&lt;p&gt;　　Jekyll是一个简单的博客形态的静态站点生成机器，我们能通过Jekyll方便的搭建并维护一个博客。     &lt;br /&gt;
　　除此之外，Jekyll也可以运行在GitHub Page上，也就是说，你可以使用GitHub的服务来搭建你的项目页面、博客或者网站，而且是完全免费的。&lt;/p&gt;

&lt;p&gt;　　下面来看一下如何在Windows系统下正确安装Jekyll：&lt;/p&gt;

&lt;h3 id=&quot;1安装ruby&quot;&gt;1.安装Ruby&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://rubyinstaller.org/downloads/&quot;&gt;下载链接&lt;/a&gt;    &lt;br /&gt;
版本的差异可能会出现一些不确定的问题，笔者在这方面吃过亏，这里建议下载2.3.3版本。
&lt;img src=&quot;\images\posts\2018\0301\RubyInstall.png&quot; alt=&quot;Ruby安装&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;
	安装过程一路”Next”，需要注意记得勾选下图中的选项。
&lt;img src=&quot;\images\posts\2018\0301\RubyInstall_1.png&quot; alt=&quot;Ruby安装选项&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;
	安装完成后输入”ruby -v”，显示版本号表明安装成功。
&lt;img src=&quot;\images\posts\2018\0301\RubyInstall_2.png&quot; alt=&quot;Ruby安装成功&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2安装devkit&quot;&gt;2.安装DevKit&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://rubyinstaller.org/downloads/&quot;&gt;下载链接&lt;/a&gt;，地址与Ruby下载地址相同，注意要安装与Ruby版本相对应的DevKit
&lt;img src=&quot;\images\posts\2018\0301\DevKitInstall.png&quot; alt=&quot;DevKit下载&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　进入到DevKit的安装目录，在命令行窗口输入下列命令：  &lt;br /&gt;
　　&lt;code class=&quot;highlighter-rouge&quot;&gt;ruby dk.rb init&lt;/code&gt;  &lt;br /&gt;
　　&lt;code class=&quot;highlighter-rouge&quot;&gt;ruby dk.rb install&lt;/code&gt;  &lt;br /&gt;
&lt;img src=&quot;\images\posts\2018\0301\DevKitInstall_1.png&quot; alt=&quot;DevKit配置&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3安装jekyll&quot;&gt;3.安装Jekyll&lt;/h3&gt;
&lt;p&gt;首先检查是否安装gem:&lt;code class=&quot;highlighter-rouge&quot;&gt;gem -v&lt;/code&gt;，输入&lt;code class=&quot;highlighter-rouge&quot;&gt;gem install jekyll&lt;/code&gt;安装Jekyll（科学上网）。&lt;/p&gt;

&lt;h3 id=&quot;4启动jekyll&quot;&gt;4.启动Jekyll&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll new myblog&lt;/code&gt;  &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd myblog&lt;/code&gt;  &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;  &lt;br /&gt;
启动过程可能出现下图中的错误信息：
&lt;img src=&quot;\images\posts\2018\0301\JekyllError.png&quot; alt=&quot;Jekyll启动Error&quot; /&gt;
这是由于Jekyll启动需要的默认端口4000已被占用，只需要打开_config.yml 在最后加上一行 port:5002 (其它端口也可)问题解决&lt;/p&gt;

&lt;h3 id=&quot;参考&quot;&gt;参考&lt;/h3&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://jekyllcn.com/&quot;&gt;Jekyll官方文档&lt;/a&gt;  &lt;br /&gt;
[2] &lt;a href=&quot;http://blog.csdn.net/itmyhome1990/article/details/50443826&quot;&gt;升级Jekyll 3.0&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/jekyll-install.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/jekyll-install.html</guid>
      </item>
    
      <item>
        <title>MarkDown编写小技巧</title>
        <description>&lt;p&gt;　　MarkDown的详细使用方法可参考&lt;a href=&quot;https://www.appinn.com/markdown/&quot;&gt;这里&lt;/a&gt;，下面是几个常用的小技巧。&lt;/p&gt;
&lt;h3 id=&quot;1换行&quot;&gt;1.换行&lt;/h3&gt;
&lt;p&gt;　　在行尾敲两个以上的空格，然后回车&lt;/p&gt;

&lt;h3 id=&quot;2首行缩进&quot;&gt;2.首行缩进&lt;/h3&gt;
&lt;p&gt;　　MarkDown是不支持缩进的，将输入法切换为全角，敲两个空格，简单美观&lt;/p&gt;

&lt;h3 id=&quot;3图片缩放&quot;&gt;3.图片缩放&lt;/h3&gt;
&lt;p&gt;　　![描述](url){:height=”70%” width=”70%”}&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/markdown.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/markdown.html</guid>
      </item>
    
      <item>
        <title>计算着色器（Compute Shader）</title>
        <description>&lt;p&gt;　　图形处理器（Graphics Processing Unit,简称GPU）每秒能够进行数以亿次的计算，目前其已成为一种性能十分惊人的器件。通常，GPU主要用来承担实时图形渲染中的海量数学运算，然而，其潜在的计算能力也可用于处理与图形无关的任务。 &lt;br /&gt;
　　CUDA与OpenCL早已能够基于GPU实现并行计算，而OpenGL与DirectX也分别于OpenGL4.3与DirectX11版本后能够通过计算着色器的方式基于GPU实现并行计算。从图中可以看到，计算着色器是独立与通常的渲染管线的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;CUDA：NVIDIA推出的通用并行计算架构，该架构能够使用GPU解决复杂的计算问题。  &lt;br /&gt;
OpenCL：针对异构系统进行并行编程的一个全新API，能够利用GPU进行并行计算。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;\images\posts\2018\0228\pipeline.png&quot; alt=&quot;渲染管线&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-cpu与gpu&quot;&gt;1. CPU与GPU&lt;/h2&gt;
&lt;p&gt;　　前面提到了计算着色器能够基于GPU实现并行计算，从而提升计算能力，那么GPU到底独特在哪呢？为什么又比CPU的计算速度还要快？  &lt;br /&gt;
　　理解CPU与GPU之间区别的一种简单方式是比较他们是如果处理任务的：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt; 由专为顺序串行处理而优化的几个核心组成，其需要很强的通用性来处理各种不同的数据类型，除此之外逻辑判断会引入大量的分支跳转和中断处理，使得CPU的内部结构异常复杂。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt; 拥有一个由数以千计的更小、更高效的核心（专为同时处理多重任务而设计）组成的大规模并行计算架构。
&lt;img src=&quot;\images\posts\2018\0228\compare.jpg&quot; alt=&quot;CPU与GPU结构对比图&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt; 
　　其中，绿色是计算单元，橙红色是存储单元，橙黄色是控制单元。 &lt;br /&gt;
　　GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的逻辑控制并省去了Cache；而CPU不仅被Cache占据了大量空间，还有很复杂的控制逻辑和诸多优化电路，相比之下计算能力只是GPU很小的一部分。&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 28 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/first-blog.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/first-blog.html</guid>
      </item>
    
      <item>
        <title>机器视觉：Asymmetry Problem in Computer Vision</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;自然法则无时不刻不给予着人类以对称性的恩惠，从一片树叶到人类自身，其形态都是对称的。对称性的特性，大大减轻了人类的记忆和认知负担。然而，弱相互作用中互为镜像的物质的运动不对称却暗藏着自然法则对非对称性的偏爱。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在计算机视觉中，对称性是一个很好的先验，如果某一个特定的物体具备对称性的话，通过引入对称性可以提升系统的精度。常见的对称性包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;物体本身具备对称性，且这种对称性不容易受大视角变化的影响。主要应用场景为在训练诸如检测模型的时候，可以将这一信息加入到训练样本的扩增上；&lt;/li&gt;
  &lt;li&gt;相似性度量具备对称性。这种对称性常体现在设计的相似性度量准则上，A与B计算出的相似性和B与A得到的相似性是一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们的大脑深受对称性法则的影响，所以对于第一种情况，我们可以非常直观的将这一法则应用到我们的视觉模型训练中，但是对于第二种情况，特别是当相似性度量由非对称性在对称性上转化而来的时候，我们的第一意识却频频出错。计算机视觉中非对称现象和非对称相似性度量如此干扰我们的第一意识，以至于小白菜对这个问题曾做出过若干的思考，下面是小白菜结合自己的一些思考和经验做的总结和整理。&lt;/p&gt;

&lt;h3 id=&quot;局部特征匹配中的非对称问题&quot;&gt;局部特征匹配中的非对称问题&lt;/h3&gt;

&lt;p&gt;在用局部特征进行匹配的时候，比如SIFT，用A图匹配B图和用B图匹配A图，得到的匹配结果是不一样的，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1029/sift_matching_diff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这匹配的过程中，所有的度量方式比如最近邻、次近邻、几何校验等都是具备对称性的，所以很容易给我们造成一种错觉就是他们的匹配结果应该是一样的。这里面造成匹配结果不对称的根本因素在于&lt;strong&gt;A图和B图它们的局部特征数目是不相等的&lt;/strong&gt;，比如A有500个局部特征，B有800个局部特征，用500个局部特征去匹配800局部个特征和用800个局部特征去匹配500个局部特征，势必造成匹配的结果不一样。一个鲁棒的匹配算法，应该在用A匹配B和用B匹配A时都能获得比较好的匹配结果，以避免单一结果匹配较好的情形，像下面的匹配算法就不是一种好的匹配算法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1029/false_sift_matching.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在设计匹配算法的时候，我们应&lt;strong&gt;避免我们的算法出现单一匹配好的情形，以提高匹配的鲁棒性&lt;/strong&gt;。如需获取上图匹配结果，可以访问&lt;a href=&quot;https://github.com/willard-yuan/covdet.git&quot;&gt;covdet&lt;/a&gt;获取。&lt;/p&gt;

&lt;h3 id=&quot;logo识别中的非对称问题&quot;&gt;Logo识别中的非对称问题&lt;/h3&gt;

&lt;p&gt;Logo的识别问题，可以分为两类:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于检测的方式&lt;/li&gt;
  &lt;li&gt;基于检索的方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于检测方式的缺点在于新的Logo样式来临时，会面临训练样本如何获取以及人工标注的问题，虽然可以自动通过往图片上贴Logo的方式来构造训练样本，但这种方式容易造成过拟合；基于检索的方式的优点在于，它不会面临训练样本获取和标注的问题，能够以较快的速度响应新Logo检测的请求，但是基于Logo检索的方式，面临一个很大的问题就是尺寸不对称性。&lt;/p&gt;

&lt;p&gt;这种尺寸不对称性体现在：对于待检测的图片，Logo所占的区域是很小的，通常区域面积占总体面积比的5%都不到，这样就导致提取的Logo区域的特征（比如局部特征）被非Logo区域的特征给“淹没”掉，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1029/logo-example.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图这个图选取得不是很合适，因为背景比较干净，Logo区域的特征还是其了比较大的作用。对于一般的情况，由于Logo区域的特征被非Logo区域的特征给淹没掉，从而使得在Logo库里检索的时候，在top@K（K取得比较小）里面比较难以检索到相关的Logo，而且即便是做重排，也比较难以将最相关的Logo排到最前面。&lt;/p&gt;

&lt;p&gt;针对这种由于尺寸不对称性造成的有用特征（信号）被无用信号淹没的问题，很难在不对检索精度造成较大影响的前提下找到比较有效的解决办法。如果要剔除无用信号造成的干扰，一种比较好的方法是先进行粗略的检测，这一步不要求检测的准确率很高，只要保证召回率很高即可，然后可以根据定位到的框提取特征，再进行检索以及校验。这种方式由于剔除了非相关区域特征的干扰，所以准确率和召回率通常能够得到较好的保证，唯一不足的是，引入了检测，而检测势必要求对数据进行标注（半自动）。不过总体来说，这仍然是一种非常不错的方法。&lt;/p&gt;

&lt;h3 id=&quot;pq中的非对称距离&quot;&gt;PQ中的非对称距离&lt;/h3&gt;

&lt;p&gt;在此前的文章&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;中，小白菜曾对PQ做过比较详细的介绍，这里对PQ中非对称距离的计算做一详述。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/PQ_search_zpskgugtocx.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，非对称距离计算方式（红框标示）在计算查询向量$x$到库中某一样本$y$之间的距离时，并不需要对查询向量$x$自身进行量化，而是直接计算查询向量$x$到量化了的$y$之间的距离，这种距离计算方式可以确保非距离计算方式以更大的概率保证计算的距离更接近于真实的距离（与对称距离计算方式相比较），而且这种方式比对称距离计算方式在实施的过程中，来得更直接，因为我们不需要对查询向量进行量化，而是直接计算到对应子段之间的距离，然后采用查表的方式获取到查询向量到库中所有样本的距离。这种通过查表的方式，是PQ能够加速距离计算的核心。&lt;/p&gt;

&lt;p&gt;非对称性的应用，在这里展示了它有利的一面，通过将&lt;strong&gt;非对称性延展到相似性度量上，可以进一步缩小量化造成的损失&lt;/strong&gt;。既然谈到了PQ的思想，我们还可以对PQ的改进做一下的延拓。&lt;/p&gt;

&lt;h4 id=&quot;polysemous-codes&quot;&gt;Polysemous Codes&lt;/h4&gt;

&lt;p&gt;PQ的改进版本很多，比如&lt;a href=&quot;kaiminghe.com/publications/pami13opq.pdf&quot;&gt;OPQ&lt;/a&gt;, Jegou等人又在PQ的基础上对PQ计算距离的过程中做了进一步加速，提出了&lt;a href=&quot;https://arxiv.org/abs/1609.01882&quot;&gt;Polysemous Codes&lt;/a&gt;（中译为“一词多义编码”，为何叫Polysemous Codes，容小白菜慢慢道来)。&lt;/p&gt;

&lt;p&gt;前面已经提到，即便是采用查表的方式，仍然还是要查表挨个对库中的每个样本计算到查询向量之间的距离，这种距离能不能转换成汉明距离的计算？正如上图所示的，对于每个向量，我们可以知道它对应的量化索引编码，如果这个量化索引编码本身就是一种汉明编码，那么我们就可以直接用通过计算汉明距离来得到粗排序的结果，然后再对topK的结果计算ADC距离，Polysemous Codes的动机正是如此，实际上Polysemous Codes不仅充当了量化索引编码，还充当了一个快速过滤的作用。&lt;/p&gt;

&lt;p&gt;应该说，对于所有的相似性度量距离，汉明距离计算从效率上来讲，是最快的。在faiss的项目wiki的&lt;a href=&quot;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)&quot;&gt;re-filtering PQ codes with polysemous codes&lt;/a&gt;有结论：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is about 6x faster to compare codes with Hamming distances than to use a product quantizer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是计算一次PQ的距离，跟计算一次汉明距离计算相比，会慢6倍左右，说明如果能把PQ编码赋予汉明编码的意义的话，距离的计算会提升6倍，这个提升还是非常巨大的。Polysemous Codes的实现已在Faiss中&lt;a href=&quot;https://github.com/facebookresearch/faiss/blob/master/IndexPQ.h#L67&quot;&gt;Polysemous Codes Implementation&lt;/a&gt;。下图解释一下“Polysemous Codes”的“Polysemous”，即一词多义：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1202/Polysemous_Codes.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Polysemous codes are compact representations of vectors that can be comparedeither with product quantization (222M distance evaluations per second per core for 8-byte codes) or as binary codes (1.19G distances per second). To obtain this property, we optimize the assignment of quantization indexes to bits such that closest centroidshave a small Hamming distance. 【摘自&lt;a href=&quot;https://arxiv.org/abs/1609.01882&quot;&gt;Polysemous Codes&lt;/a&gt;】&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在标准的PQ编码中，编码仅指代对应的量化索引编码，即由哪个类中心来近似该子段向量，比如上面的左图，4位的二进制编码仅表示类中心编码（编号），除此之外，无任何其他的意义；而在Polysemous Codes中，如右图所示，它不仅能表示类中心的编码（编号），而且它还是一种汉明编码，从这里可以看到，该编码包含了两种特性，我们既可以计算PQ距离，又可以计算汉明距离，因而该编码被称为“Polysemous Codes”是非常妥帖的。由于计算汉明距离更高效，而Polysemous Codes又是一种汉明码，我们可以先用汉明距离进行粗排序，在采用PQ距离进行重排。&lt;/p&gt;

&lt;p&gt;Polysemous Codes所拥有的这两种特性，不能不感叹它是极其优雅美丽的。&lt;/p&gt;

&lt;h3 id=&quot;累计最小最大距离非对称问题&quot;&gt;累计最小（最大）距离非对称问题&lt;/h3&gt;

&lt;p&gt;在度量两个集合的相似性的时候，累计最小（最大）距离是一个比较好用的相似性度量距离。假设两个集合分别为$X = \lbrace x_t, i=1 \dots n \rbrace$和$Y = \lbrace y_t, j=1 \dots m \rbrace$，则$X$和$Y$集合的相似性可以通过累计最小（最大）距离来度量，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
S(X, Y) = \sum_{i=1}^{i=n} \sum_{j=1}^{j=m} d_\min(x_i, y_j)
\end{equation}&lt;/p&gt;

&lt;p&gt;至于$d$选取何种距离，我们可以根据自己的应用场景来定夺，小白菜自己一般喜欢使用余弦相似度，因为该距离的计算最终可以转换成内积。累计最小（最大）距离应用的场景这里可以列举一二：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部特征构成的两个集合，这里的局部特征不限于传统的局部特征，还可以是CNN构造的局部特征；&lt;/li&gt;
  &lt;li&gt;两个视频序列分别对视频帧提取全局特征，构成的两个视频帧特征集合，使用累计最小（最大）距离我们可以得到两个视频的相似性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;累计最小（最大）距离通常也是一种非对称性距离，在两个集合样本数量相差得比较大时，这种非对称性表现得非常明显。以上面所说的第二个例子为例，当两个视频由于某种不是很合理的取帧方式，导致两个视频取的帧数目相差比较大时，计算出的$S(X, Y)$和$S(Y, X)$会相差得很大，必然会导致其中的一个得到的相似度比较小。对于这种情形，在此相似性度量方式下，并没有特别好的办法来改善，所以在取帧的时候，尽量使两者的数目相当。&lt;/p&gt;

&lt;p&gt;总体来说，累计最小（最大）距离对于度量两个集合的相似性是个不错的距离度量。下面讲讲对这个距离度量的计算速度优化问题。比如，我们要计算$X$和$Y$两个集合的相似性，并且每个集合有1000个元素，我们是要一个一个遍历计算找最小值然后相加吗？&lt;/p&gt;

&lt;p&gt;显然这种计算方式太慢。前面提到过，在对$d$相似性度量的选取上，小白菜喜欢使用余弦相似性，矩阵乘法使得我们可以避免掉挨个遍历循环，通过矩阵相乘得到$X$和$Y$中各个样本与样本之间的相似性后，我们可以对相似性矩阵进行排序，然后求和相加即可得到$S(X, Y)$，即$X$和$Y$之间的相似性。&lt;/p&gt;

&lt;p&gt;总结一下，在本篇博文中，分别从下面4个方面对计算机视觉中的非对称问题进行了探讨：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部特征匹配中的非对称问题&lt;/li&gt;
  &lt;li&gt;Logo识别中的非对称问题&lt;/li&gt;
  &lt;li&gt;PQ中的非对称距离&lt;/li&gt;
  &lt;li&gt;累计最小（最大）距离非对称问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;中间穿插了对PQ的拓展Polysemous Codes的拓展。后面如遇其他计算机视觉中的非对称性问题，会同步到这里。&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/asymmetry-problem-in-computer-vision.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/asymmetry-problem-in-computer-vision.html</guid>
      </item>
    
      <item>
        <title>图像检索：Fisher Information Matrix and Fisher Kernel</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://wiki.mbalib.com/wiki/%E7%BD%97%E7%BA%B3%E5%BE%B7%C2%B7%E8%B4%B9%E9%9B%AA&quot;&gt;罗纳德·费雪&lt;/a&gt;（Sir Ronald Aylmer Fisher, FRS，1890.2.17－1962.7.29)，现代统计学与现代演化论的奠基者之一，安德斯·哈尔德称他是“一位几乎独自建立现代统计科学的天才”，理查·道金斯则认为他是“达尔文最伟大的继承者”。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2015/1002/Ronald_Aylmer_Fisher.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;局部特征作为一种强鲁棒性的特征，其与全局特征构成了CV领域图像内容描述的基础。相比于全局特征，局部特征往往在对低层共有模式的表达上可以做到更细的粒度（关于局部和全局在视觉认知上的作用机制，强烈推荐阅读&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale_space&quot;&gt;尺度空间理论&lt;/a&gt;），但同时也引发了新的问题，即&lt;strong&gt;特征处理效率低、存储大等方面的问题&lt;/strong&gt;。因而需要将局部特征经过某种编码方式，最终表示成一种紧凑的全局特征表示。&lt;/p&gt;

&lt;p&gt;Fisher Vector作为连接单向连接局部特征到全局表示的三大特征编码方法之一（另外两种编码方式见&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;图像检索：BoF、VLAD、FV三剑客&lt;/a&gt;），无论是在学术研究领域还是在工业实际应用上，都具有很重要的地位。下面内容是小白菜对Fisher Vector中的Fisher信息矩和Fisher核的数学推导及对应物理意义的总结整理。&lt;/p&gt;

&lt;h2 id=&quot;fisher信息矩和fisher核&quot;&gt;Fisher信息矩和Fisher核&lt;/h2&gt;

&lt;p&gt;在空间$\chi$中，某样本$X$存在$T$个观测量，记为$X = \lbrace x_t, t=1 \dots T \rbrace$。对应到图像上，样本$X$为图像$I$提取到的$T$个$D$维的局部描述子，比如SIFT。设$\mu_\lambda$为概率密度函数，该函数包含有$M$个参数，即$\lambda = [\lambda_1, \dots, \lambda_M]$。根据生成式原理，空间$\chi$中的元素$X$可以由概率密度函数进行建模。在统计学上，分数函数（score funciton）可以由对数似然函数的梯度给出，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
G^X_\lambda = \nabla log \mu_\lambda(X)
\label{sfun}
\end{equation}&lt;/p&gt;

&lt;p&gt;上式对数函数的梯度，在数学形式上为对数似然函数的一阶偏导，它描述了每一个参数$\lambda_i$对该生成式过程的贡献度，换言之，该分数函数$G^X_\lambda$描述了生成式模型$\mu_\lambda$为了更好的拟合数据$X$，该模型中的参数需要做怎样的调整。又因为$G^X_\lambda \in R^M $是一个维度为$M$维的向量，所以该分数函数的维度仅依赖于$\lambda $中参数的数目$M$, 而于观测样本的数目$T$无关。此外，一般情况下，该分数函数的期望$E[ G^X_\lambda ] = 0 $，这一点对于下面讲到的Fisher信息矩物理意义的得到非常重要。&lt;/p&gt;

&lt;p&gt;根据信息几何理论，含参分布$\Gamma = \lbrace \mu_\lambda, \lambda \in \Lambda \rbrace$可以视为一个黎曼流形$M_\Lambda$，其局部度量方式可以由Fisher信息矩(Fisher Information Matrix, FIM）$F_\lambda \in R^{M \times M}$:&lt;/p&gt;

&lt;p&gt;\begin{equation}
F_{\lambda} = E_{ x \sim \mu_\lambda } [ G^X_\lambda (G^X_\lambda)^T ]
\label{fim}
\end{equation}&lt;/p&gt;

&lt;p&gt;从上式可以看到，Fisher信息矩是分数函数的二阶矩。在一般条件下很容易证明（注意$E[ G^X_\lambda ] = 0 $）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
F_{\lambda} &amp;= E_{ x \sim \mu_\lambda } [ G^X_\lambda (G^X_\lambda)^T ] \\\\
            &amp;= E[(G^X_\lambda)^2] \\\\
            &amp;= E[(G^X_\lambda)^2] - E[ G^X_\lambda ]^2 \\\\
            &amp;= Var[G^X_\lambda]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;从上式可以看到，Fisher信息矩是用来估计最大似然估计（Maximum Likelihood Estimate, MLE）的方程的方差。它直观的表述就是，在独立性假设的条件下，随着收集的观测数据越来越多，这个方差由于是一个相加的形式，因而Fisher信息矩也就变的越来越大，也就表明得到的信息越来越多。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：此处引用了&lt;a href=&quot;https://www.zhihu.com/question/26561604&quot;&gt;fisher information 的直观意义是什么?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于两组不同的观察样本$X$和$Y$，Jaakkola和Haussler提出了使用Fisher核来度量它们之间的相似性，其数学表达形式为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
K_FK(X, Y) = (G^X_\lambda)^T F_\lambda^{-1} G^X_\lambda
\label{fk}
\end{equation}&lt;/p&gt;

&lt;p&gt;又因为$F_{\lambda}$是半正定的，所以其逆矩阵是存在的。使用cholesky分解可以得到$F_\lambda^{-1} = (L_\lambda)^T L_\lambda$，上式可以写成内积的表示形式：&lt;/p&gt;

&lt;p&gt;\begin{equation}
K_FK(X, Y) = (\wp^X_\lambda)^T \wp^Y_\lambda
\label{fk1}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中,
\begin{equation}
\wp^X_\lambda = L_\lambda G^X_\lambda = L_\lambda \nabla log \mu_\lambda(X)
\label{wp_lambda}
\end{equation}&lt;/p&gt;

&lt;p&gt;上式是$L_\lambda$对$G^X_\lambda$的归一化，我们将$\wp^X_\lambda$称为Fisher向量，该Fisher向量$\wp^X_\lambda$等于梯度向量$G^X_\lambda$的维度，又由于$G^X_\lambda$的维度仅与概率密度函数的参数数目$M$有关，所以空间$\chi$中任意样本$X$的$T$个观测量最终都可以表示成一固定维度的向量。通过使用$\wp^X_\lambda$算子，使得非线性核相似性度量问题转化为线性问题。这种变换带来的一个明显的优势是，在分类的时候可以采用更高效的线性分类器。&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Oct 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/fim-fisher-kernel.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/fim-fisher-kernel.html</guid>
      </item>
    
      <item>
        <title>深度学习：Neural Network Layers Understanding</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;我想做又应该做的事，都会做到；我想做却不应做的事，都会戒掉。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;inner-product-layer&quot;&gt;Inner Product Layer&lt;/h2&gt;

&lt;p&gt;Inner Product Layer即全连接层，对于IP层的理解，可以简单的将其视为矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;1*N&lt;/code&gt;和矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;N*M&lt;/code&gt;相乘后得到&lt;code class=&quot;highlighter-rouge&quot;&gt;1*M&lt;/code&gt;的维度向量。&lt;/p&gt;

&lt;p&gt;举个简单的例子，比如输入全连接层的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;3*56*56&lt;/code&gt;维度的数据，假设未知的权重维度为&lt;code class=&quot;highlighter-rouge&quot;&gt;N*M&lt;/code&gt;，假设全连接层的输出为&lt;code class=&quot;highlighter-rouge&quot;&gt;num_ouput = 4096&lt;/code&gt;，为了计算全连接层的输出，全连接层会将输入的数据&lt;code class=&quot;highlighter-rouge&quot;&gt;3*56*56&lt;/code&gt; reshape 成为&lt;code class=&quot;highlighter-rouge&quot;&gt;1*N&lt;/code&gt;的形式，即&lt;code class=&quot;highlighter-rouge&quot;&gt;1x(56x56x3) = 1x9408&lt;/code&gt;，所以：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N = 9408
M = num_ouput = 4096
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;由此，我们做了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;(1x9408)&lt;/code&gt;矩阵和&lt;code class=&quot;highlighter-rouge&quot;&gt;(9408x4096)&lt;/code&gt;矩阵的乘法。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;num_output&lt;/code&gt;的值改变成为100，则做的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;(1x9408)&lt;/code&gt;矩阵和&lt;code class=&quot;highlighter-rouge&quot;&gt;(9408x100)&lt;/code&gt;矩阵的乘法。&lt;strong&gt;Inner Product layer（常被称为全连接层）将输入视为一个vector，输出也是一个vector（height和width被设为1）&lt;/strong&gt;。下面是IP层的示意图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/fcgemm_corrected.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/&quot;&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;增大&lt;code class=&quot;highlighter-rouge&quot;&gt;num_output&lt;/code&gt;会使得模型需要学习的权重参数增加。IP层一个典型的例子：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;有了上面对IP层的理解，对&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/inner_product_layer.cpp&quot;&gt;caffe inner_product_layer.cpp&lt;/a&gt;中Forward的理解就比较自然了。下面是Caffe的IP层在CPU上的实现：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InnerProductLayer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutable_cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;caffe_cpu_gemm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transpose_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;bottom_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_term_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;caffe_cpu_gemm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bias_multiplier_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面完成矩阵与矩阵相乘的函数是&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&amp;lt;Dtype&amp;gt;&lt;/code&gt;（见&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp&quot;&gt;math_functions.cpp&lt;/a&gt;)，&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&lt;/code&gt;函数矩阵相乘的具体数学表示形式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   C=alpha*TransA(A)*TransB(B) + beta*C\\\\
\end{equation}&lt;/script&gt;

&lt;p&gt;上式中&lt;code class=&quot;highlighter-rouge&quot;&gt;TransX&lt;/code&gt;是对&lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;做的一种矩阵变换，比如转置、共轭等，具体是&lt;code class=&quot;highlighter-rouge&quot;&gt;cblas.h&lt;/code&gt;中定义的为枚举类型。在&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp&quot;&gt;math_functions.cpp&lt;/a&gt;中，除了定义矩阵与矩阵相乘的&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&lt;/code&gt;外，还定义了矩阵与向量的相乘，具体的函数为&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemv&lt;/code&gt;，其数学表示形式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   C=alpha*TransA(A)*y + beta*y\\\\
\end{equation}&lt;/script&gt;

&lt;p&gt;上面表达式中，&lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;是向量，不是标量。&lt;/p&gt;

&lt;h4 id=&quot;参考&quot;&gt;参考&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/&quot;&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35788873/what-is-the-output-of-fully-connected-layer-in-cnn&quot;&gt;What is the output of fully connected layer in CNN?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/seven_first/article/details/47378697&quot;&gt;caffe_cpu_gemm函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/u011762313/article/details/47361571&quot;&gt;Caffe学习：Layers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/layers.html&quot;&gt;Caffe Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;gemm&quot;&gt;GEMM&lt;/h2&gt;

&lt;p&gt;在上面的IP层中，我们已经涉及到了GEMM的知识，在这一小节里面，不妨对该知识点做一个延伸。&lt;/p&gt;

&lt;p&gt;GEMM是BLAS (Basic Linear Algebra Subprograms)库的一部分，该库在1979年首次创建。为什么GEMM在深度学习中如此重要呢？我们可以先来看一个图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/gemm_cup_gpu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;http://daggerfs.com/&quot;&gt;Yangqing Jia&lt;/a&gt;的&lt;a href=&quot;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.pdf&quot;&gt;thesis&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上图是采用AlexNet对CNN网络中不同layer GPU和CPU的时间消耗，从更底层的实现可以看到CNN网络的主要时间消耗用在了FC (for fully-connected)和Conv (for convolution)，而FC和Conv在实现上都将其转为了矩阵相乘的形式。举个例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/cnn_gemm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;http://arxiv.org/pdf/1410.0759.pdf&quot;&gt;cuDNN: Efficient Primitives for Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面Conv在Caffe中具体实现的时候，会将每一个小的patch拉成一个向量，很多patch构成的向量会构成一个大的矩阵，同样的对于多个卷积核展成一个矩阵形式，从而将图像的卷积转成了矩阵与矩阵的相乘（更形象化的解释参阅&lt;a href=&quot;https://www.zhihu.com/question/28385679/answer/44297845&quot;&gt;在 Caffe 中如何计算卷积？&lt;/a&gt;）。上面可以看到在FC和Conv上消耗的时间GPU占95%，CPU上占89%。因而GEMM的实现高效与否对于整个网络的效率有很大的影响。&lt;/p&gt;

&lt;p&gt;那么什么是GEMM呢？GEMM的全称是GEneral Matrix to Matrix Multiplication，正如其字面意思所表达的，GEMM即表示两个输入矩阵进行相乘，得到一个输出的矩阵。两个矩阵在进行相乘的时候，通常会进行百万次的浮点运算。对于一个典型网络中的某一层，比如一个256 row&lt;em&gt;1152 column的矩阵和一个1152 row&lt;/em&gt;192 column的矩阵，二者相乘57 million (256 x 1152 x 192)的浮点运算。因而，通常我们看到的情形是，一个网络在处理一帧的时候，需要几十亿的FLOPs（Floating-point operations per second，每秒浮点计算）。&lt;/p&gt;

&lt;p&gt;既然知道了GEMM是限制整个网络时间消耗的主要部分，那么我们是不是可以对GEMM做优化调整呢？答案是否定的，GEMM采用Fortran编程语言实现，经过了科学计算编程人员几十年的优化，性能已经极致，所以很难再去进一步的优化，在Nvidia的论文&lt;a href=&quot;http://arxiv.org/pdf/1410.0759.pdf&quot;&gt;cuDNN: Efficient Primitives for Deep Learning&lt;/a&gt;中指出了还存在着其他的一些方法，但是他们最后采用的还是改进的GEMM版本实现。GEMM可匹敌的对手是傅里叶变换，将卷积转为频域的相乘，但是由于在图像的卷积中存在strides，使得傅里叶变换方式很难保持高效。&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/neural-network-layers-understanding.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/neural-network-layers-understanding.html</guid>
      </item>
    
      <item>
        <title>知行手记：毕业一周年</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;陷于思，简于情&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;六月的校园充满了告别与不舍，不管愿意与否，时间的巨浪一年又一年的将那些停靠在港湾的船儿推向未知的大海。这一切对于时间而言，不过是社会周期性的轮转，但对于每一条驶入大海的船而言，与大地相隔离开的那一刻，却是个体生命进程中最具仪式感的时刻之一——从此，风吹雨打日晒，或行进，或随流。&lt;/p&gt;

&lt;p&gt;如果说生活需要一些仪式感来刺激日复一日的平凡乃至碌碌无为，那么对于行将走完的六月之末，最大的仪式感便是自己已告别学生时代一周年了。所谓记录也是留念，博文余下部分是小白菜对自己半年来（去年下半年的状态&lt;a href=&quot;http://yongyuan.name/blog/year-turned-back.html&quot;&gt;2016年，归零清空&lt;/a&gt;）的&lt;strong&gt;思想&lt;/strong&gt;、&lt;strong&gt;情感&lt;/strong&gt;的一个总结，希望自己力行笃志，勿忘初心。&lt;/p&gt;

&lt;h2 id=&quot;陷于思&quot;&gt;陷于思&lt;/h2&gt;

&lt;p&gt;对于一个内心极其不安分的人而言，日复一复时间的轮转和实体空间中的条条框框都是一种桎梏。对此，小白菜深有同感并以此为然。小白菜深知自己不是一个很安分的人，虽然在现实生活中循规蹈矩，可是主导自我的精神体系却像一匹狂奔在草原的野马，总想着&lt;strong&gt;去浪荡、去经历、去过一种行无定处的生活&lt;/strong&gt;。这样一种精神体系的主导后果就是间歇性情绪低落症，如果在某一段较长时间内，在生活找不到有所期待的事物或者工作中无小有成就的喜感，便会陷入连日的情绪低落症，以至于在心里盘算着要不要make some changes。关于情绪低落甚而陷入沮丧，&lt;a href=&quot;https://www.douban.com/people/81194074/&quot;&gt;ErbB4不麋鹿&lt;/a&gt;说:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我发现一个人生活时避免沮丧的方法就是始终有期待，比如很期待程序算出来的结果，或者很期待尝试的新菜谱的味道，或者是某天的活动，正在追的剧，正在看的书的情节发展，和盘算已久的旅行等等。如果没有期待，就会怀疑自己生活的意义，虽然思考意义没什么不好，但没有积极情绪的反弹，是很难走出来的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;想必间歇性情绪低落症也不是小白菜独自面临的问题，上面&lt;a href=&quot;https://www.douban.com/people/81194074/&quot;&gt;ErbB4不麋鹿&lt;/a&gt;所说的有所期待，说到底便是一个被讨论了无数次的hope问题。关于hope，小白菜觉得最好的解释，莫过于《肖申克的救赎》中Andy Dufresne所说的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hope is a good thing, maybe the best of things and no good thing ever dies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这一点上，史铁生的&lt;a href=&quot;https://book.douban.com/subject/1136988/&quot;&gt;命若琴弦&lt;/a&gt;也有很深刻的探讨。作为一个上班族，在陷入日复一复上班的轮子里后，也难免在某一段时间内走着走着会陷入一定的迷茫，虽然工作上也有很明确的目标，有很多很多的问题要解，可是如果将时间拉得更长一些，比如两年、三年、五年后自己想实现什么样的一个大目标或者想成为什么样的一个人，便也只好用什么努力实现财务自由或者成为特定领域的专家来搪瓷。这样贴近“地气”的目标虽然是好，可是纯碎地靠这种方式去驱动，却也总是陷入自驱力的泥潭，精神状态一直无法摆脱周期性余弦振动的困扰。这一困扰的根本，小白菜以为在于信仰的缺失，只是小白菜却从未曾找到过，以至于小白菜的微信花名一直是“小白菜在寻找”。&lt;/p&gt;

&lt;p&gt;关于信仰，大部分人包括小白菜在内，都很难说是有信仰的。我们可能有一个、两个或是更多的理想，然而这些零星般的理想，却不过是我们生命中的或大或小的一部分，它可能在较长的一段时间内对我们很重要，但是在下一个循环结中，它对于当前的自己而言已经无关痛痒了。信仰却不同，信仰是一个人用尽自己全部的生命进程去追求、捍卫的一项崇高的事业或者精神准则，以至于到了&lt;strong&gt;无法坚持信仰的那一刻，他会选择宝贵生命来表达他最后的忠诚&lt;/strong&gt;，譬如&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%87%8B%E5%BB%A3%E5%BE%B7&quot;&gt;Thich Quang Duc&lt;/a&gt;为了捍卫宗教的信仰而选择火的殉道。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/Thich_Quang_Duc.jpg&quot; alt=&quot;Thich Quang Duc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个有信仰的人是幸运地，无论在坚持自我、坚持信仰的道路上他都会获得极大的精神满足，就像&lt;a href=&quot;https://book.douban.com/subject/1858513/&quot;&gt;月亮与六便士&lt;/a&gt;中的思特里克兰德一样，他必须画画，就像溺水的人必须挣扎。关于信仰的种种探讨以及如何找到自己的信仰，小白菜自感功力浅薄，所谓历事勤读，答案也许潜藏于生活中，也有可能存在于某本书中，只是需要借由时间去参透。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;无善无恶心之体，有善有恶意之动，知善知恶是良知，为善去恶是格物&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;小白菜愿用一生的时间多读、多思，去理解、践行阳明心学，构建、稳固并完善自己的精神体系。&lt;/p&gt;

&lt;h2 id=&quot;简于情&quot;&gt;简于情&lt;/h2&gt;

&lt;p&gt;如果要用三句简短的话来概括友情、爱情和亲情这半年来的状态，小白菜以为最好的总结莫过如此：&lt;strong&gt;友情逐步沉淀，爱情迟迟未来，亲情依然照旧&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;友情&quot;&gt;友情&lt;/h3&gt;

&lt;p&gt;对于友情，小白菜也越来越赞同并接纳这样一种观点：闲来无事勿相扰。本科四年，读研三年，再到如今工作一年，从曾经的闹哄哄(各种八卦群)到现在各自回归自我平静的生活，与其说是情感的趋于平淡，倒不如说是各自生活和个人精神的沉淀。&lt;strong&gt;那些曾经美好的相处渐渐沉于记忆，而过于平淡的终将忘却，最后内心惦记并念念不忘的可能也就三两知心友人，或志趣相投、或曾经一块儿奋斗&lt;/strong&gt;。他们从不曾随意打扰过小白菜的生活，亦很少有这样的机会让小白菜去帮助过什么(问题他们总能自己搞定)。&lt;/p&gt;

&lt;p&gt;天各一方，岁月相伴，我们探讨技术，分享工作和生活的心得，心心相惜彼此的才能而乐于在合适的机会面前推举对方，小白菜以为，这就是最好最成熟的友情。&lt;/p&gt;

&lt;h3 id=&quot;爱情&quot;&gt;爱情&lt;/h3&gt;

&lt;p&gt;在绝大多数人都会步入的这条路上，爱情对于小白菜而言，还处于故事的起点。虽不愿随随便便找个人共度来日时光，却再也不肯多在上面做些功课。小白菜匠心情怀、锲而不舍、自信满满，但在爱情这条路上，在经历了一次失意后被打趴得止步不前。此后，在爱情这条路上，小白菜是如此的敏感，以致于蜷缩得像一只浑身是刺心也逐渐石化的刺猬，&lt;strong&gt;不轻易靠近人，也不想被无关的人接近&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;所以，从大学到如今工作满一年，时至8年，在历经了一场长达4年之久的单相思后，未曾再有过心动的妹子。也许，小白菜已经失去了喜欢一个人的能力，再或者，小白菜压根就不知道如何去爱一个人。&lt;/p&gt;

&lt;p&gt;过度的执着于爱情的相处理念，未必是一件好事。也只有到了“这把年纪”，才深切体会到爱情成为一种欠债和任务的无奈（苦笑）。蔡永康说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;15岁觉得游泳难，放弃游泳，到18岁遇到一个你喜欢的人约你去游泳，你只好说“我不会耶”。18岁觉得英文难，放弃英文，28岁出现一个很棒但要会英文的工作，你只好说“我不会耶”。人生前期越嫌麻烦，越懒得学，后来就越可能错过让你动心的人和事，错过新风景。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;再说小白菜对于爱情的理解，在小白菜还小的时候，曾看到过这样一句话：真正的爱，不再于轰轰烈烈，也不再于信誓旦旦，而是在情感的全心投入中，加以责任的相伴，完成平静的相守。十几年后的今天，当小白菜一字一句敲出这段话来时，对于它的理解增加了几许。如果要用影像来表达爱情的释义，小白菜以为&lt;a href=&quot;https://movie.douban.com/subject/2129039/&quot;&gt;飞屋环游记&lt;/a&gt;和&lt;a href=&quot;https://movie.douban.com/subject/5327189/&quot;&gt;伦敦一家人&lt;/a&gt;足以。&lt;/p&gt;

&lt;h3 id=&quot;亲情&quot;&gt;亲情&lt;/h3&gt;

&lt;p&gt;特别喜欢归有光先生的&lt;a href=&quot;https://www.douban.com/group/topic/3096552/&quot;&gt;项脊轩志&lt;/a&gt;，一个人的小阁子，自言自语，哀而不伤，时至如今，仍能诵读一二：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;借书满架，偃仰啸歌，冥然兀坐，万簌有声；而庭阶寂寂，小鸟时来啄食，人至不去。三五之夜，明月半墙，桂影斑驳，风移影动，珊珊可爱。&lt;br /&gt;
······&lt;br /&gt;
庭有枇杷树，吾妻死之年所手植也，今已亭亭如盖矣。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;“庭阶寂寂，小鸟时来啄食，人至不去。三五之夜，明月半墙，桂影斑驳，风移影动，珊珊可爱”，像流水的生活、似脉脉的亲情，在喧闹的五道口，仍能在内心最柔弱的找到一处沉寂的&lt;a href=&quot;https://book.douban.com/subject/1865089/&quot;&gt;瓦尔登湖&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;小白菜特别希望老妈和老爸对生活充满了那么一点点的情调，譬如养一盆绿萝，在收到老姐送给的康乃馨时不是再磕磕叨叨又说乱花钱了······这一切希冀的小期待对于那些经历过大锅饭的父辈们来说，实在是苛求。&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/graduate-after-one-year.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/graduate-after-one-year.html</guid>
      </item>
    
      <item>
        <title>图像检索：layer选择与fine-tuning性能提升验证</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;这个世界上肯定有另一个我，做着我不敢做的事，过着我想过的生活。一个人逛街，一个人吃饭，一个人旅行，一个人做很多事。极致的幸福，存在于孤独的深海。在这样日复一日的生活里，我逐渐和自己达成和解。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为迁移学习的一种，finetune能够将general的特征转变为special的特征，从而使得转移后的特征能够更好的适应目标任务，而图像检索最根本的问题，仍在于如何在目标任务上获得更好的特征表达(共性与可区分性)。一种很自然的方式便是在特定的检索任务上，我们对imageNet学得的general的特征通过finetune的方式，使得表达的特征能够更好的适应我们的检索任务。在&lt;a href=&quot;https://arxiv.org/abs/1610.07940&quot;&gt;End-to-end Learning of Deep Visual Representations for Image Retrieval&lt;/a&gt;和&lt;a href=&quot;https://www.computer.org/csdl/trans/tp/preprint/07867860.pdf&quot;&gt;
Collaborative Index Embedding for Image Retrieval&lt;/a&gt;中已经很清楚的指出，通过基本的classification loss的finetune的方式，能够较大幅度的提高检索的mAP。因此，在本篇博文中，小白菜针对检索，主要整理了下面四个方面的内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN网络中哪一层最适合于做图像检索&lt;/li&gt;
  &lt;li&gt;基于pre-trained模型做图像检索几种典型的特征表示方法&lt;/li&gt;
  &lt;li&gt;抽取网络任意层的特征&lt;/li&gt;
  &lt;li&gt;数据增强(Data Augmentation)&lt;/li&gt;
  &lt;li&gt;VGGNet16网络模型fine-tuning实践&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在采用深度学习做检索的时候，上面四方面的问题和知识基本都回避不了，因此，小白菜以为，掌握这四方面的内容显得非常有必要。&lt;/p&gt;

&lt;h2 id=&quot;特征表达layer选择&quot;&gt;特征表达layer选择&lt;/h2&gt;

&lt;p&gt;在AlexNet和VGGNet提出伊始，对于检索任务，小白菜相信，在使用pre-trained模型抽取特征的时候，我们最最自然想到的方式是抽取全连接层中的倒数第一层或者倒数第二层的特征，这里说的倒数第一层或者倒数第二层并没有具体指明是哪一层（fcx、fcx_relux、fcx_dropx），以VggNet16网络为例，全连接层包含两层，fc6和fc7，因此我们很自然想到的网络层有fc6、fc6_relu6、fc7、fc7_relu7甚至fc6_drop6和fc7_drop7（后面会说明fc6_drop6和fc6_relu6是一样的，以及fc7_drop7和fc7_relu7也是一样的），所以即便对于我们最最自然最最容易想到的方式，也面临layer的选择问题。为此，我们以VGGNet16网络为例，来分析CNN网络的语义层(全连接层)选择不同层作为特征做object retrieval的mAP的影响。&lt;/p&gt;

&lt;p&gt;小白菜选取fc6、fc6_relu6、fc7、fc7_relu7这四层语义层的特征，在&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/&quot;&gt;Oxford Building&lt;/a&gt;上进行实验，评价指标采用mAP，mAP的计算采用Oxford Building提供的计算mAP代码&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp&quot;&gt;compute_ap.cpp&lt;/a&gt;，下表是fc6、fc6_relu6、fc7、fc7_relu7对应的mAP。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(128维)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(4096维)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(4096维, 未做PCA)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc7_relu7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;44.72%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.11%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41.08%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;45.03%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19.67%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41.18%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc6_relu6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43.62%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;23.0%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43.34%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;45.9%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19.47%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;44.78%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;从上表可以看到，直接采用pre-trained模型抽取语义层的特征，在Oxford Building上取得的结果在45%左右，同时我们还可以看出，选取fc6、fc6_relu6、fc7、fc7_relu7对结果的影响并不大。这个结果只能说非常的一般，在基于pre-trained模型做object retrieval的方法中，比如&lt;a href=&quot;https://arxiv.org/abs/1512.04065&quot;&gt;Cross-dimensional Weighting for Aggregated Deep Convolutional Features&lt;/a&gt;、&lt;a href=&quot;https://arxiv.org/abs/1511.05879&quot;&gt;Particular object retrieval with integral max-pooling of CNN activations&lt;/a&gt;以及&lt;a href=&quot;https://arxiv.org/abs/1611.01640&quot;&gt;What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?&lt;/a&gt;指出，选用上层的语义层其实是不利于object retrieval，因为上层的语义层丢失了object的空间信息，并且从实验的角度说明了&lt;strong&gt;选取中间层的特征&lt;/strong&gt;更利于object retrieval。&lt;/p&gt;

&lt;p&gt;实际上，在选取中间层来表达特征的过程中，我们可以去掉全连接层，从而使得我们可以摆脱掉输入图像尺寸约束(比如224*224)的约束，而保持原图大小的输入。通常，图像分辨率越大，对于分类、检测等图像任务是越有利的。因而，从这一方面讲，&lt;strong&gt;选取上层的全连接层作为特征，并不利于我们的object retrieval任务&lt;/strong&gt;。一种可能的猜想是，上层全连接层的语义特征，应该更适合做全局的相似。&lt;/p&gt;

&lt;p&gt;虽然中间层更适合于做object retrieval，但是在选用中间层的feature map作为raw feature的时候，我们面临的一个主要问题是：如何将3d的tensor转成一个有效的向量特征表示？下面小白菜主要针对这一主要问题总结几种典型的特征表示方法，以及对中间层特征选择做一些探讨与实验。&lt;/p&gt;

&lt;h2 id=&quot;基于pre-trained模型做object-retrieval几种典型的特征表示&quot;&gt;基于pre-trained模型做Object Retrieval几种典型的特征表示&lt;/h2&gt;

&lt;h3 id=&quot;sum-pooling&quot;&gt;SUM pooling&lt;/h3&gt;

&lt;p&gt;基于SUM pooling的中层特征表示方法，指的是针对中间层的任意一个channel（比如VGGNet16, pool5有512个channel），将该channel的feature map的所有像素值求和，这样每一个channel得到一个实数值，N个channel最终会得到一个长度为N的向量，该向量即为SUM pooling的结果。&lt;/p&gt;

&lt;h3 id=&quot;ave-pooling&quot;&gt;AVE pooling&lt;/h3&gt;

&lt;p&gt;AVE pooling就是average pooling，本质上它跟SUM pooling是一样的，只不过是将像素值求和后还除以了feature map的尺寸。小白菜以为，&lt;strong&gt;AVE pooling可以带来一定意义上的平滑，可以减小图像尺寸变化的干扰&lt;/strong&gt;。设想一张224&lt;em&gt;224的图像，将其resize到448&lt;/em&gt;448后，分别采用SUM pooling和AVE pooling对这两张图像提取特征，我们猜测的结果是，SUM pooling计算出来的余弦相似度相比于AVE pooling算出来的应该更小，也就是AVE pooling应该稍微优于SUM pooling一些。&lt;/p&gt;

&lt;h3 id=&quot;max-pooling&quot;&gt;MAX pooling&lt;/h3&gt;

&lt;p&gt;MAX pooling指的是对于每一个channel（假设有N个channel），将该channel的feature map的像素值选取其中最大值作为该channel的代表，从而得到一个N维向量表示。小白菜在&lt;a href=&quot;https://github.com/willard-yuan/flask-keras-cnn-image-retrieval/blob/master/extract_cnn_vgg16_keras.py&quot;&gt;flask-keras-cnn-image-retrieval&lt;/a&gt;中采用的正是MAX pooling的方式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/max_pooling_zpsglehm2jv.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;from &lt;a href=&quot;https://www.slideshare.net/xavigiro/contentbased-image-retrieval-d2l6-insightdcu-machine-learning-workshop-2017&quot;&gt;Day 2 Lecture 6 Content-based Image Retrieval&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面所总结的SUM pooling、AVE pooling以及MAX pooling，这三种pooling方式，在小白菜做过的实验中，MAX pooling要稍微优于SUM pooling、AVE pooling。不过这三种方式的pooling对于object retrieval的提升仍然有限。&lt;/p&gt;

&lt;h3 id=&quot;mop-pooling&quot;&gt;MOP pooling&lt;/h3&gt;

&lt;p&gt;MOP Pooling源自&lt;a href=&quot;https://arxiv.org/abs/1403.1840&quot;&gt;Multi-scale Orderless Pooling of Deep Convolutional Activation Features&lt;/a&gt;这篇文章，一作是Yunchao Gong，此前在搞&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;哈希&lt;/a&gt;的时候，读过他的一些论文，其中比较都代表性的论文是ITQ，小白菜还专门写过一篇笔记&lt;a href=&quot;http://yongyuan.name/blog/itq-hashing.html&quot;&gt;论文阅读：Iterative Quantization迭代量化&lt;/a&gt;。MOP pooling的基本思想是多尺度与VLAD(VLAD原理可以参考小白菜之前写的博文&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;图像检索：BoF、VLAD、FV三剑客&lt;/a&gt;)，其具体的pooling步骤如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/mop_cnn_zpstvgo29kk.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;from &lt;a href=&quot;https://arxiv.org/abs/1403.1840&quot;&gt;Multi-scale Orderless Pooling of Deep Convolutional Activation Features&lt;/a&gt;&lt;br /&gt;
Overview  of  multi-scale  orderless  pooling  for  CNN  activations  (MOP-CNN). Our proposed feature is a concatenation of the feature vectors from three levels: (a)Level 1, corresponding to the 4096-dimensional CNN activation for the entire 256&lt;em&gt;256image; (b) Level 2, formed by extracting activations from 128&lt;/em&gt;128 patches and VLADpooling them with a codebook of 100 centers; (c) Level 3, formed in the same way aslevel 2 but with 64*64 patches.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体地，在L=1的尺度下，也就是全图，直接resize到256&lt;em&gt;256的大小，然后送进网络，得到第七层全连接层4096维的特征；在L=2时，使用128&lt;/em&gt;128(步长为32)的窗口进行滑窗，由于网络的图像输入最小尺寸是256&lt;em&gt;256，所以作者将其上采样到256&lt;/em&gt;256，这样可以得到很多的局部特征，然后对其进行VLAD编码，其中聚类中心设置为100，4096维的特征降到了500维，这样便得到了50000维的特征，然后将这50000维的特征再降维得到4096维的特征；L=3的处理过程与L=2的处理过程一样，只不过窗口的大小编程了64*64的大小。&lt;/p&gt;

&lt;p&gt;作者通过实验论证了MOP pooling这种方式得到的特征一定的不变性。基于这种MOP pooling小白菜并没有做过具体的实验，所以实验效果只能参考论文本身了。&lt;/p&gt;

&lt;h3 id=&quot;crow-pooling&quot;&gt;CROW pooling&lt;/h3&gt;

&lt;p&gt;对于Object Retrieval，在使用CNN提取特征的时候，我们所希望的是在有物体的区域进行特征提取，就像提取局部特征比如SIFT特征构&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;BoW、VLAD、FV向量&lt;/a&gt;的时候，可以采用MSER、Saliency等手段将SIFT特征限制在有物体的区域。同样基于这样一种思路，在采用CNN做Object Retrieval的时候，我们有两种方式来更细化Object Retrieval的特征：一种是先做物体检测然后在检测到的物体区域里面提取CNN特征；另一种方式是我们通过某种权重自适应的方式，加大有物体区域的权重，而减小非物体区域的权重。CROW pooling ( &lt;a href=&quot;https://arxiv.org/abs/1512.04065&quot;&gt;Cross-dimensional Weighting for Aggregated Deep Convolutional Features&lt;/a&gt; )即是采用的后一种方法，通过构建Spatial权重和Channel权重，CROW pooling能够在&lt;strong&gt;一定程度上&lt;/strong&gt;加大感兴趣区域的权重，降低非物体区域的权重。其具体的特征表示构建过程如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/crow_zpsaejbmsln.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其核心的过程是Spatial Weight和Channel Weight两个权重。Spatial Weight具体在计算的时候，是直接对每个channel的feature map求和相加，这个Spatial Weight其实可以理解为saliency map。我们知道，通过卷积滤波，响应强的地方一般都是物体的边缘等，因而将多个通道相加求和后，那些非零且响应大的区域，也一般都是物体所在的区域，因而我们可以将它作为feature map的权重。Channel Weight借用了IDF权重的思想，即对于一些高频的单词，比如“the”，这类词出现的频率非常大，但是它对于信息的表达其实是没多大用处的，也就是它包含的信息量太少了，因此在BoW模型中，这类停用词需要降低它们的权重。借用到Channel Weight的计算过程中，我们可以想象这样一种情况，比如某一个channel，其feature map每个像素值都是非零的，且都比较大，从视觉上看上去，白色区域占据了整个feature map，我们可以想到，这个channel的feature map是不利于我们去定位物体的区域的，因此我们需要降低这个channel的权重，而对于白色区域占feature map面积很小的channel，我们认为它对于定位物体包含有很大的信息，因此应该加大这种channel的权重。而这一现象跟IDF的思想特别吻合，所以作者采用了IDF这一权重定义了Channel Weight。&lt;/p&gt;

&lt;p&gt;总体来说，这个Spatial Weight和Channel Weight的设计还是非常巧妙的，不过这样一种pooling的方式只能在一定程度上契合感兴趣区域，我们可以看一下Spatial Weight*Channel Weight的热力图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/sp_example_zps4ntos3ok.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面可以看到，权重大的部分主要在塔尖部分，这一部分可以认为是discriminate区域，当然我们还可以看到，在图像的其他区域，还有一些比较大的权重分布，这些区域是我们不想要的。当然，从小白菜可视化了一些其他的图片来看，这种crow pooling方式并不总是成功的，也存在着一些图片，其权重大的区域并不是图像中物体的主体。不过，从千万级图库上跑出来的结果来看，crow pooling这种方式还是可以取得不错的效果。&lt;/p&gt;

&lt;h3 id=&quot;rmac-pooling&quot;&gt;RMAC pooling&lt;/h3&gt;

&lt;p&gt;RMAC pooling的池化方式源自于&lt;a href=&quot;https://arxiv.org/pdf/1511.05879&quot;&gt;Particular object retrieval with integral max-pooling of CNN activations&lt;/a&gt;，三作是Hervé Jégou(和Matthijs Douze是好基友)。在这篇文章中，作者提出来了一种RMAC pooling的池化方式，其主要的思想还是跟上面讲过的MOP pooling类似，采用的是一种变窗口的方式进行滑窗，只不过在滑窗的时候，不是在图像上进行滑窗，而是在feature map上进行的(极大的加快了特征提取速度)，此外在合并local特征的时候，MOP pooling采用的是VLAD的方式进行合并的，而RMAC pooling则处理得更简单(简单并不代表效果不好)，直接将local特征相加得到最终的global特征。其具体的滑窗方式如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/rmac_pooling_zpsigvxjjud.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;from &lt;a href=&quot;https://www.slideshare.net/xavigiro/contentbased-image-retrieval-d2l6-insightdcu-machine-learning-workshop-2017&quot;&gt;Day 2 Lecture 6 Content-based Image Retrieval&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;图中示意的是三种窗口大小，图中‘x’代表的是窗口的中心，对于每一个窗口的feature map，论文中采用的是MAX pooling的方式，在L=3时，也就是采用图中所示的三种窗口大小，我们可以得到20个local特征，此外，我们对整个fature map做一次MAX pooling会得到一个global特征，这样对于一幅图像，我们可以得到21个local特征(如果把得到的global特征也视为local的话)，这21个local特征直接相加求和，即得到最终全局的global特征。论文中作者对比了滑动窗口数量对mAP的影响，从L=1到L=3，mAP是逐步提升的，但是在L=4时，mAP不再提升了。实际上RMAC pooling中设计的窗口的作用是定位物体位置的(CROW pooling通过权重图定位物体位置)。如上图所示，在窗口与窗口之间，都是一定的overlap，而最终在构成global特征的时候，是采用求和相加的方式，因此可以看到，那些重叠的区域我们可以认为是给予了较大的权重。&lt;/p&gt;

&lt;p&gt;上面说到的20个local特征和1个global特征，采用的是直接合并相加的方式，当然我们还可以把这20个local特征相加后再跟剩下的那一个global特征串接起来。实际实验的时候，发现串接起来的方式比前一种方式有2%-3%的提升。在规模100万的图库上测试，RMAC pooling能够取得不错的效果，跟Crow pooling相比，两者差别不大。&lt;/p&gt;

&lt;p&gt;上面总结了6中不同的pooling方式，当然还有很多的pooling方式没涵盖不到，在实际应用的时候，小白菜比较推荐采用RMAC pooling和CROW pooling的方式，主要是这两种pooling方式效果比较好，计算复杂度也比较低。&lt;/p&gt;

&lt;h2 id=&quot;抽取网络任意层的特征&quot;&gt;抽取网络任意层的特征&lt;/h2&gt;

&lt;p&gt;在上面一节中，我们频繁的对网络的不同层进行特征的抽取，并且我们还提到fc6_dropx和fc6_relux是一样的（比如fc7_drop7和fc7_relu7是一样的），这一节主要讲述使用Caffe抽取网络任意一层的特征，并从实验的角度验证fc6_dropx和fc6_relux是一样的这样一个结论。&lt;/p&gt;

&lt;p&gt;为了掌握Caffe中网络任意一层的特征提取，不妨以一个小的题目来说明此问题。题目内容为：给定VGGNet16网络，抽取fc7、fc7_relu7以及fc7_drop7层的特征。&lt;br /&gt;
求解过程：VggNet16中&lt;a href=&quot;https://gist.githubusercontent.com/ksimonyan/211839e770f7b538e2d8/raw/0067c9b32f60362c74f4c445a080beed06b07eb3/VGG_ILSVRC_16_layers_deploy.prototxt&quot;&gt;deploy.txt&lt;/a&gt;中跟fc7相关的层如下：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc6&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 4096
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;relu7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: RELU
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;drop7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DROPOUT
  dropout_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    dropout_ratio: 0.5
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果使用&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['fc7'].data[0]&lt;/code&gt;，我们抽取的特征是fc7层的特征，也就是上面：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc6&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 4096
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这一层的特征，仿照抽取fc7特征抽取的代码，我们很自然的想到抽取relu7的特征为&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['relu7'].data[0]&lt;/code&gt;和drop7的特征为&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['drop7'].data[0]&lt;/code&gt;，但是在运行的时候提示不存在&lt;code class=&quot;highlighter-rouge&quot;&gt;relu7&lt;/code&gt;层和&lt;code class=&quot;highlighter-rouge&quot;&gt;drop7&lt;/code&gt;层，原因是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To elaborate a bit further: The layers drop7 and relu7 have the same blobs as top and bottom, respectively, and as such the blobs’ values are manipulated directly by the layers. The advantage is saving a bit of memory, with the drawback of not being able to read out the state the values had before being fed through these two layers. It is simply not saved anywhere. If you want to save it, you can just create another two blobs and re-wire the layers a bit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;摘自&lt;a href=&quot;https://groups.google.com/forum/#!topic/caffe-users/766VK11Cnwo&quot;&gt;Extracting ‘relu’ and ‘drop’ blobs with pycaffe&lt;/a&gt;，因而，为了能够提取relu7和drop7的特征，我们需要将上面的配置文件做些更改，主要是将layers里面的字段换下名字(在finetune模型的时候，我们也需要做类似的更改字段的操作)，这里小白菜改成了：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc6&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 4096
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_relu7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_relu7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: RELU
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_drop7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_drop7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DROPOUT
  dropout_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    dropout_ratio: 0.5
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;经过这样的修改后，我们使用&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['fc7_relu7'].data[0]&lt;/code&gt;即可抽取到relu7的特征，使用&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['fc7_drop7'].data[0]&lt;/code&gt;可抽取到drop7的特征。&lt;/p&gt;

&lt;h2 id=&quot;数据增强&quot;&gt;数据增强&lt;/h2&gt;

&lt;p&gt;有了上面&lt;strong&gt;CNN网络中哪一层最适合于做图像检索&lt;/strong&gt;、&lt;strong&gt;基于pre-trained模型做图像检索几种典型的特征表示方法&lt;/strong&gt;以及&lt;strong&gt;抽取网络任意层的特征&lt;/strong&gt;三方面的知识储备后，在具体fine-tuning网络进行图像检索实验前，还有一节很重要(虽然我们都很熟悉)内容，即数据增强(Data Augmentation)。数据增强作用有二：一是均衡样本，比如某些类别只有几张图片，而有的类别有上千张，如果不做均衡，分类的时候计算的分类准确率会向样本充足的类别漂移；二是提高网络对于样本旋转、缩放、模糊等的鲁棒性，提高分类精度。在实际工作中，我们拿到了图像数据样本对采用深度学习模型而言，经常是不充足且不均衡的，所以这一步数据的前置处理是非常重要的。&lt;/p&gt;

&lt;p&gt;在正式开始数据增强之前，对&lt;strong&gt;图片进行异常检测是非常重要的&lt;/strong&gt;，其具体的异常表现在图片内容缺失、图片不可读取或者可以读取但数据出现莫名的问题，举个例子，比如通过爬虫爬取的图片，可能上半部分是正常的，下半分缺失一片灰色。因此，如果你不能确保你训练的图片数据都是可正常读取的时候，最好对图片做异常检测。假设你的训练图片具有如下层级目录：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  imgs_diff tree &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 2
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
├── 0
│   ├── 1227150149_1.jpg
│   ├── 1612549977_1.jpg
│   ├── 1764084098_1.jpg
│   ├── 1764084288_1.jpg
│   └── 1764085346_1.jpg
└── 1
    ├── 1227150149_1.jpg
    ├── 1612549977_1.jpg
    ├── 1764084098_1.jpg
    ├── 1764084288_1.jpg
    └── 1764085346_1.jpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;下面是小白菜参考网上资料写的图片异常检测代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;glob&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;check_pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;imgs_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata/*/*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgs_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#img = Image.open(img_path)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#if img.verify() is not None or img is None:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#    print img_path&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;IOError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;SystemError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;通过上面的图片异常检测，我们可以找到那些不可读取或者读取有问题的图片找出来，这样在我们使用Caffe将图片转为LMDB数据存储的时候，不会出现图片读取有问题的异常。在图片异常检测完成后，便可以继续后面的数据增强了。&lt;/p&gt;

&lt;p&gt;在小白菜调研的数据增强工具中，小白菜以为，最好用的还是&lt;a href=&quot;preprocessing&quot;&gt;Keras&lt;/a&gt;中的数据增强。Keras数据增强部分包含在&lt;a href=&quot;https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py&quot;&gt;image.py&lt;/a&gt;，通过类&lt;code class=&quot;highlighter-rouge&quot;&gt;ImageDataGenerator&lt;/code&gt;可以看到Keras包含了对图片的不同处理，下面是小白菜基于Keras写的数据增强脚本，假设你的图像数据目录结构具有如下结构：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  imgs_dataset tree &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 2
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
├── 0
│   ├── 1227150149_1.jpg
│   ├── 1612549977_1.jpg
│   ├── 1764084098_1.jpg
│   ├── 1764084288_1.jpg
│   └── 1764085346_1.jpg
└── 1
    ├── 1227150149_1.jpg
    ├── 1612549977_1.jpg
    ├── 1764084098_1.jpg
    ├── 1764084288_1.jpg
    └── 1764085346_1.jpg
    ....
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;数据增强的脚本如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#-*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Author: yuanyong.name&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.preprocessing.image&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataGenerator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_to_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_to_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_img&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;datagen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataGenerator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rotation_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;width_shift_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;height_shift_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;shear_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;zoom_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;horizontal_flip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fill_mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'nearest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sub_dirs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;walk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_perAug&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_perAug&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#print &quot;Aug: %s&quot; % img_path&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# this is a PIL image, please replace to your own file path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_to_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# this is a Numpy array with shape (3, 150, 150)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# this is a Numpy array with shape (1, 3, 150, 150)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datagen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_to_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_prefix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'aug'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_perAug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# otherwise the generator would loop indefinitely&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
    
&lt;span class=&quot;c&quot;&gt;# delete extra aug images&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aug_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aug_'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aug_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_del&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aug_imgs_del&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aug_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_del&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aug_imgs_del&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;target_path_dir&lt;/code&gt;是设置你训练数据集目录的，&lt;code class=&quot;highlighter-rouge&quot;&gt;num_wanted&lt;/code&gt;是设置每一个类你想要多少个样本(包括原始样本在内)，比如在&lt;a href=&quot;https://arxiv.org/abs/1404.1777&quot;&gt;Neural Codes for Image Retrieval&lt;/a&gt;中的数据集landmark数据集，里面的类别少的样本只有9个，多的类别样本有上千个，样本部分极其不均匀，上面代码中，小白菜设置得是&lt;code class=&quot;highlighter-rouge&quot;&gt;num_wanted=800&lt;/code&gt;，即让每一个类别的样本数据控制在800左右(多出来的类别样本不会删除，比如有的类别可能原始样本就有1000张，那么不会对该类做数据增强)。&lt;/p&gt;

&lt;p&gt;在这一节，小白菜总结了为什么要数据增强，或者说数据增强有什么好处，然后提供了两个脚本：图片异常检测脚本&lt;a href=&quot;https://github.com/willard-yuan/util-scripts/blob/master/py/dl/check_image.py&quot;&gt;check_image.py&lt;/a&gt;和数据增强脚本&lt;a href=&quot;https://github.com/willard-yuan/util-scripts/blob/master/py/dl/keras_imgAug.py&quot;&gt;keras_imgAug.py&lt;/a&gt;。有了前面三部分的知识和本节数据前置处理的实践，我们终于可以进行fine-tuning了。&lt;/p&gt;

&lt;h2 id=&quot;vggnet16网络模型fine-tuning实践&quot;&gt;VGGNet16网络模型fine-tuning实践&lt;/h2&gt;

&lt;p&gt;在实际中，用CNN做分类任务的时候，一般我们总是用在ImageNet上预训练好的模型去初始化待训练模型的权重，也就是不是train from scratch，主要原因有二：一是在实际中很难获取到大量样本(即便是做了数据增强)；二是加快模型训练的速度。因而，针对检索这个任务，我们也采用fine-tuning的方式，让在ImageNet上预训练的模型迁移到我们自己的特定的数据集上，从而提升特征在检索任务上的表达能力。下面小白菜以fine-tuning Neural Codes提供的数据集为例，比较详实的总结一个完整的fine-tuning过程。&lt;/p&gt;

&lt;p&gt;在fine-tuning之前，我们先追问一个简单的问题和介绍一下Neural Codes提供的landmark数据集。追问的这个问题很简单：为什么几乎所有的做检索的论文中，使用的都是AlexNet、VGGNet16（偶尔会见到一两篇使用ResNet101）网络模型？难道做研究的只是关注方法，使用AlexNet、VGGNet、ResNet或者Inception系列只是替换一下模型而已？小白菜曾也有过这样的疑问，但是对这些网络测试下来，发觉VGGNet在做基于预训练模型特征再表达里面效果是最好的，对于同一个方法，使用ResNet或Inception系列，其mAP反而没有VGGNet的高。至于为什么会这样，小白菜也没有想明白(如果有小伙伴知道，请告知)，我们就暂且把它当做一条经验。&lt;/p&gt;

&lt;p&gt;我们再对Neural Codes论文里提供的landmark数据集做一个简单的介绍。该数据集共有680类，有的类别样本数据至于几个，多则上千，样本分布极其不均匀。不过这不是问题，通过第4节介绍的数据增强和提供的脚本，我们可以将每个类别的样本数目控制在800左右。同时，我们可以使用下面脚本将每个类别所在目录的文件夹名字命名为数字：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata'&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 该目录下有很多子文件夹，每个子文件夹是一个类别&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;walk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;得到清洗后的数据后，下面分步骤详解使用VGGNet16来fine-tuning的过程。&lt;/p&gt;

&lt;h3 id=&quot;切分数据集为train和val&quot;&gt;切分数据集为train和val&lt;/h3&gt;

&lt;p&gt;经过上面重命名后的数据集文件夹目录具有如下形式：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  imgs_dataset tree &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 2
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
├── 0
│   ├── 1227150149_1.jpg
│   ├── 1612549977_1.jpg
│   ├── 1764084098_1.jpg
│   ├── 1764084288_1.jpg
│   └── 1764085346_1.jpg
│   ....
└── 1
    ├── 1227150149_1.jpg
    ├── 1612549977_1.jpg
    ├── 1764084098_1.jpg
    ├── 1764084288_1.jpg
    └── 1764085346_1.jpg
    ....
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;我们需要将数据集划分为train数据集和val数据集，注意val数据集并不单纯只是在训练的时候测试一下分类的准确率。为了方便划分数据集，小白菜写了如下的脚本，可以很方便的将数据集划分为train数据集和val数据集：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;glob&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;classes_path&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata/*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 该目录下有很多子文件夹，每个子文件夹train_samples = []&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;imgs_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'//*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 训练集占60%&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;imgs_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lmdb/train.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lmdb/val.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行上面脚本，会在lmdb目录下(事先需要建立lmdb目录)生成两个文本文件，分别为&lt;code class=&quot;highlighter-rouge&quot;&gt;train.txt&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val.txt&lt;/code&gt;，对于为训练数据集和验证数据集。&lt;/p&gt;

&lt;h3 id=&quot;图片转成lmdb存储&quot;&gt;图片转成lmdb存储&lt;/h3&gt;

&lt;p&gt;为了提高图片的读取效率，Caffe将图片转成lmdb进行存储，在上面得到&lt;code class=&quot;highlighter-rouge&quot;&gt;train.txt&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val.txt&lt;/code&gt;后，我们需要借助caffe的&lt;code class=&quot;highlighter-rouge&quot;&gt;convert_imageset&lt;/code&gt;工具将图片resize到某一固定的尺寸，同时转成为lmdb格式存储。下面是小白菜平时使用的完成该任务的一个简单脚本&lt;code class=&quot;highlighter-rouge&quot;&gt;crop.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/home/yuanyong/caffe/build/tools/convert_imageset &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         &lt;span class=&quot;nt&quot;&gt;--resize_height&lt;/span&gt; 256 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         &lt;span class=&quot;nt&quot;&gt;--resize_width&lt;/span&gt; 256 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         / &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         lmdb/train.txt &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         lmdb/train_lmdb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;运行两次，分别对应于&lt;code class=&quot;highlighter-rouge&quot;&gt;train.txt&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val.txt&lt;/code&gt;。运行完后，会在lmdb目录下生成&lt;code class=&quot;highlighter-rouge&quot;&gt;train_lmdb&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val_lmdb&lt;/code&gt;两目录，为了校验转成lmdb存储是否成功，我们最好分别进入这两个目录下看看文件的大小以做简单的验证。&lt;/p&gt;

&lt;h3 id=&quot;生成均值文件&quot;&gt;生成均值文件&lt;/h3&gt;

&lt;p&gt;对于得到的&lt;code class=&quot;highlighter-rouge&quot;&gt;train_lmdb&lt;/code&gt;，我们在其上计算均值。具体地，使用Caffe的&lt;code class=&quot;highlighter-rouge&quot;&gt;compute_image_mean&lt;/code&gt;工具：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;/build/tools/compute_image_mean lmdb/train_lmdb lmdb/mean.binaryproto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;在lmdb目录下即可得到均值文件&lt;code class=&quot;highlighter-rouge&quot;&gt;mean.binaryproto&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;修改train_valprototxt和solverprototxt&quot;&gt;修改train_val.prototxt和solver.prototxt&lt;/h3&gt;

&lt;p&gt;针对VGGNET16网络，在fine-tuning的时候，我们通常将最后的分类层的学习率设置得比前面网络层的要大，一般10倍左右。当然，我们可以结合自己的需要，可以将前面层的学习率都置为0，这样网络在fine-tuning的时候，只调整最后一层分类层的权重；在或者我们分两个阶段去做fine-tuning，第一阶段只fine-tuning最后的分类层，第二阶段正常的fine-tuning所有的层(包含最后的分类层)。同时，我们还需要对最后一层分类层重新换个名字，并且对应的分类输出类别也需要根据自己数据集的分类类别数目做调整，下面小白菜给出自己在fine-tuning Neural Codes的landmark数据集上train_val.prototxt的前面输入部分和后面分类的部分：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: &lt;span class=&quot;s2&quot;&gt;&quot;VGG_ILSVRC_16_layers&quot;&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DATA
  include &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    phase: TRAIN
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 transform_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    crop_size: 224
    mean_file: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/mean.binaryproto&quot;&lt;/span&gt;
    mirror: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 data_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/train_lmdb&quot;&lt;/span&gt;
    batch_size: 64
    backend: LMDB
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DATA
  include &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    phase: TEST
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 transform_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    crop_size: 224
    mean_file: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/mean.binaryproto&quot;&lt;/span&gt;
    mirror: &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 data_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/val_lmdb&quot;&lt;/span&gt;
    batch_size: 52
    backend: LMDB
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

...
...
...

layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  blobs_lr: 10          &lt;span class=&quot;c&quot;&gt;# 学习率是前面网络层是10倍&lt;/span&gt;
  blobs_lr: 20          &lt;span class=&quot;c&quot;&gt;# 学习率是前面网络层是10倍&lt;/span&gt;
  weight_decay: 1
  weight_decay: 0
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 680     &lt;span class=&quot;c&quot;&gt;# 共680类&lt;/span&gt;
    weight_filler &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;gaussian&quot;&lt;/span&gt;
      std: 0.01
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    bias_filler &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;constant&quot;&lt;/span&gt;
      value: 0
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;loss&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: SOFTMAX_LOSS
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;loss/loss&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: ACCURACY
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;
  include: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; phase: TEST &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面配置测试输入的时候，&lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;设置的是52，这个设置非常重要，我们一定要保证这个设置的&lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;跟solver.prototxt里面设置的&lt;code class=&quot;highlighter-rouge&quot;&gt;test_iter&lt;/code&gt;乘起来等于测试样本数目。下面再看看solver.prototxt这个文件：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net: &lt;span class=&quot;s2&quot;&gt;&quot;train_val.prototxt&quot;&lt;/span&gt;
test_iter: 4005
test_interval: 5000
base_lr: 0.001
lr_policy: &lt;span class=&quot;s2&quot;&gt;&quot;step&quot;&lt;/span&gt;
gamma: 0.1
stepsize: 20000
display: 1000
max_iter: 50000
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: &lt;span class=&quot;s2&quot;&gt;&quot;../models/snapshots_&quot;&lt;/span&gt;
solver_mode: GPU
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;比较重要的5个需要调整参数分别是&lt;code class=&quot;highlighter-rouge&quot;&gt;test_iter&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;test_interval&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;stepsize&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;momentum&lt;/code&gt;。&lt;code class=&quot;highlighter-rouge&quot;&gt;test_iter&lt;/code&gt;怎么设置上面已经介绍。&lt;code class=&quot;highlighter-rouge&quot;&gt;test_interval&lt;/code&gt;表示迭代多少次进行一次验证及测试，&lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;表示基础学习率，一般要比正常训练时的学习率要小，&lt;code class=&quot;highlighter-rouge&quot;&gt;stepsize&lt;/code&gt;跟步长相关，可以简单的理解为步长的分母，&lt;code class=&quot;highlighter-rouge&quot;&gt;momentum&lt;/code&gt;按推荐设置为0.9就可以。&lt;/p&gt;

&lt;p&gt;设置完上面的train_val.prototxt和solver.prototxt后，便可以开始正式fine-tuning了。&lt;/p&gt;

&lt;h3 id=&quot;正式fine-tuning&quot;&gt;正式fine-tuning&lt;/h3&gt;

&lt;p&gt;借助Caffe工具集下的caffe，我们只需要简单的执行下面命令即可完成网络的fine-tuning:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;/build/tools/caffe train &lt;span class=&quot;nt&quot;&gt;-solver&lt;/span&gt;  solver.prototxt &lt;span class=&quot;nt&quot;&gt;-weights&lt;/span&gt; http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel &lt;span class=&quot;nt&quot;&gt;-gpu&lt;/span&gt; 0,1 | &lt;span class=&quot;nb&quot;&gt;tee &lt;/span&gt;log.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中&lt;code class=&quot;highlighter-rouge&quot;&gt;-gpu&lt;/code&gt;后面接的数字表示GPU设备的编号，这里我们使用了0卡和1卡，同时我们将训练的日志输出到log.txt里面。&lt;/p&gt;

&lt;h3 id=&quot;测试&quot;&gt;测试&lt;/h3&gt;

&lt;p&gt;完成了在Neural Codes的landmark数据集上的fine-tuning后，我们使用经过了fine-tuning后的模型在Oxford Building数据集上mAP提升了多少。为了方便对比，我们仍然提取fc6的特征，下面是不做ft(fine-tuning)和做ft的结果对比：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(128维)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fc6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;45.9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fc6+ft&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;60.2%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;可以看到，经过fine-tuning，mAP有了较大幅度的提升。从而也从实验的角度验证了对于检索任务，在数据允许的条件，对预训练模型进行fine-tuning显得非常的有必要。&lt;/p&gt;

&lt;h3 id=&quot;复现本文实验&quot;&gt;复现本文实验&lt;/h3&gt;

&lt;p&gt;如想复现本文实验，可以在这里&lt;a href=&quot;https://github.com/willard-yuan/cnn-cbir-benchmark/tree/master/fc_retrieval&quot;&gt;fc_retrieval&lt;/a&gt;找到相应的代码。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;在本篇博文中，小白菜就5个方面的问题展开了总结和整理，分别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN网络中哪一层最适合于做图像检索&lt;/li&gt;
  &lt;li&gt;基于pre-trained模型做图像检索几种典型的特征表示方法&lt;/li&gt;
  &lt;li&gt;抽取网络任意层的特征&lt;/li&gt;
  &lt;li&gt;数据增强(Data Augmentation)&lt;/li&gt;
  &lt;li&gt;VGGNet16网络模型fine-tuning实践&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个文章的基本组织结构依照典型的工科思维方式进行串接，即从理论到实践。&lt;/p&gt;
</description>
        <pubDate>Tue, 30 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/layer-selection-and-finetune-for-cbir.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/layer-selection-and-finetune-for-cbir.html</guid>
      </item>
    
      <item>
        <title>图像检索：再叙ANN Search</title>
        <description>&lt;p&gt;每逢碰到这个ANN的简称，小白菜总是想到Artificial Neural Network人工神经网络，不过这里要展开的ANN并不是Artificial Neural Network，而是已被小白菜之前写过很多次的Approximate Nearest Neighbor搜索。虽然读书的那会儿，这一块的工作专注得比较多，比如哈希，也整理过一个像模像样的工具包&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;hashing-baseline-for-image-retrieval&lt;/a&gt;，以及包括KD树、PQ乘积量化等近似最近邻搜索，但这些东西放在今天小白菜的知识体系里来看，依然自以为还非常的散乱。所以借再次有专研的机会之际，再做一次整理，完善自己在索引这方面的知识体系。&lt;/p&gt;

&lt;p&gt;在具体到不同类的索引方法分类前，小白菜以为，从宏观上对ANN有下面的认知显得很有必要：&lt;strong&gt;brute-force搜索的方式是在全空间进行搜索，为了加快查找的速度，几乎所有的ANN方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历&lt;/strong&gt;。可以看到，正是因为缩减了遍历的空间大小范围，从而使得ANN能够处理大规模数据的索引。&lt;/p&gt;

&lt;p&gt;根据小白菜现有的对ANN的掌握，可以将ANN的方法分为三大类：基于树的方法、哈希方法、矢量量化方法。这三种方法里面，着重总结典型方法，其中由以哈希方法、矢量量化方法为主。&lt;/p&gt;

&lt;h2 id=&quot;基于树的方法&quot;&gt;基于树的方法&lt;/h2&gt;

&lt;p&gt;几乎所有的ANN方法都是对全空间的划分，所以基于树的方法也不例外。基于树的方法采用&lt;strong&gt;树&lt;/strong&gt;这种数据结构的方法来表达对全空间的划分，其中又以KD树最为经典。下面左图是KD树对全空间的划分过程，以及用树这种数据结构来表达的一个过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/kdTree_zpshq4ywnby.PNG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对KD树选择从哪一维度进行开始划分的标准，采用的是求每一个维度的方差，然后选择方差最大的那个维度开始划分。这里有一个比较有意思的问题是：&lt;strong&gt;为何要选择方差作为维度划分选取的标准&lt;/strong&gt;？我们都知道，方差的大小可以反映数据的波动性。方差大表示数据波动性越大，选择方差最大的开始划分空间，可以使得所需的划分面数目最小，反映到树数据结构上，可以使得我们构建的KD树的树深度尽可能的小。为了更进一步加深对这一点的认识，可以以一个简单的示例图说明：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/kd_zpslmugktds.JPEG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设不以方差最大的x轴为划分面(x_var = 16.25)，而是以y轴(y_var = 0.0)轴为划分面，如图中虚线所示，可以看到，该划分使得图中的四个点都落入在同一个子空间中，从而使得该划分成为一个无效的划分，体现在以树结构上，就是多了一层无用的树深度。而以x轴为初始划分则不同(图像实线所示)，以x轴为初始划分可以得到数据能够比较均匀的散布在左右两个子空间中，从而使得整体的查找时间能够最短。注意，在实际的kd树划分的时候，并不是图中虚线所示，而是选取中值最近的点。上面示意图构建的具体kd树如下所示：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kdtree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;visualize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

                       &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;一般而言，在空间维度比较低的时候，KD树是比较高效的，当空间维度较高时，可以采用下面的哈希方法或者矢量量化方法。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;kd-trees are not suitable for efficiently finding the nearest neighbour in high dimensional spaces.&lt;br /&gt;
In very high dimensional spaces, the curse of dimensionality causes the algorithm to need to visit many more branches than in lower dimensional spaces. In particular, when the number of points is only slightly higher than the number of dimensions, the algorithm is only slightly better than a linear search of all of the points.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;哈希方法&quot;&gt;哈希方法&lt;/h2&gt;

&lt;p&gt;哈希，顾名思义，就是将连续的实值散列化为0、1的离散值。在散列化的过程中，对散列化函数(也就是哈希函数)有一定的要求。根据学习的策略，可以将哈希方法分为无监督、有监督和半监督三种类型。在评估某种哈希方法用于图像检索的检索精度时，可以使用knn得到的近邻作为ground truth，也可以使用样本自身的类别作为ground truth。所以在实际评估准确度的时候，根据ground truth的定义，这里面是有一点小的trick的。通常对于无监督的哈希图像检索方法，由于我们使用的都是未标记的数据样本，所以我们会很自然的采用knn得到的近邻作为ground truth，但是对于图像检索的这一任务时，在对哈希函数的构造过程中，通常会有“相似的样本经编码后距离尽可能的近，不相似的样本编码后则尽可能的远”这一基本要求，这里讲到的相似，指语义的相似，因而你会发现，编码的基本要求放在无监督哈希方法里，似乎与采用knn得到的近邻作为ground truth的评价方式有些南辕北辙。对无监督哈希方法的ground truth一点小的疑惑在小白菜读书的时候就心存这样的困惑，一直悬而未解。当然，在做无监督的图像哈希方法，采用样本自身的类别作为ground truth是毋庸置疑的。&lt;/p&gt;

&lt;p&gt;小白菜读书那会儿，研究了很多的哈希图像检索方法（见&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;hashing-baseline-for-image-retrieval&lt;/a&gt;），有时候总会给一些工程实践上的错觉（在今天看来是这样的），即新论文里的方法远远碾压经典的方法，那是不是在实际中这些方法就很work很好使。实践的经历告诉小白菜，还是经典的东西更靠谱，不是因为新的方法不好，而是新的事物需要经过时间的沉淀与优化。&lt;/p&gt;

&lt;p&gt;所以，这里不会对近两年的哈希方法做铺陈，而是聊一聊工程中在要使用到哈希方法的场景下一般都会选用的局部敏感哈希（Local Sensitive Hashing, LSH）。&lt;/p&gt;

&lt;h3 id=&quot;local-sensitive-hashing&quot;&gt;Local Sensitive Hashing&lt;/h3&gt;

&lt;p&gt;关于LSH的介绍，小白菜以为，&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/wiki/LSH-Primer&quot;&gt;Locality-Sensitive Hashing: a Primer&lt;/a&gt;这个讲解得极好，推荐一读。下面是小白菜结合自己的理解，提炼的一些在小白菜看来需要重点理解的知识（附上LSH划分空间示意图，在进行理解的时候可以参照改图）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/lsh_ex_zps0lryoykz.PNG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;局部敏感是啥&quot;&gt;局部敏感是啥？&lt;/h4&gt;

&lt;p&gt;当一个函数（或者更准确的说，哈希函数家族）具有如下属性的时候，我们说该哈希函数是局部敏感的：相近的样本点对比相远的样本点对更容易发生碰撞。&lt;/p&gt;

&lt;h4 id=&quot;用哈希为什么可以加速查找&quot;&gt;用哈希为什么可以加速查找？&lt;/h4&gt;

&lt;p&gt;对于brute force搜索，需要遍历数据集中的所有点，而使用哈希，我们首先找到查询样本落入在哪个cell(即所谓的桶)中，如果空间的划分是在我们想要的相似性度量下进行分割的，则查询样本的最近邻将极有可能落在查询样本的cell中，如此我们只需要在当前的cell中遍历比较，而不用在所有的数据集中进行遍历。&lt;/p&gt;

&lt;h4 id=&quot;为什么要用多表哈希&quot;&gt;为什么要用多表哈希？&lt;/h4&gt;

&lt;p&gt;对于单表哈希，当我们的哈希函数数目K取得太大，查询样本与其对应的最近邻落入同一个桶中的可能性会变得很微弱，针对这个问题，我们可以重复这个过程L次，从而增加最近邻的召回率。这个重复L次的过程，可以转化为构建L个哈希表，这样在给定查询样本时，我们可以找到L个哈希桶（每个表找到一个哈希桶），然后我们在这L个哈希表中进行遍历。这个过程相当于构建了K*L个哈希函数(注意是“相当”，不要做“等价”理解)。&lt;/p&gt;

&lt;h4 id=&quot;多表哈希中哈希函数数目k和哈希表数目l如何选取&quot;&gt;多表哈希中哈希函数数目K和哈希表数目L如何选取？&lt;/h4&gt;

&lt;p&gt;哈希函数数目K如果设置得过小，会导致每一个哈希桶中容纳了太多的数据点，从而增加了查询响应的时间；而当K设置得过大时，会使得落入每个哈希桶中的数据点变小，而为了增加召回率，我们需要增加L以便构建更多的哈希表，但是哈希表数目的增加会导致更多的内存消耗，并且也使得我们需要计算更多的哈希函数，同样会增加查询相应时间。这听起来非常的不妙，但是在K过大或过小之间仍然可以找到一个比较合理的折中位置。通过选取合理的K和L，我们可以获得比线性扫描极大的性能提升。&lt;/p&gt;

&lt;h4 id=&quot;multiprobe-lsh是为了解决什么问题&quot;&gt;Multiprobe LSH是为了解决什么问题？&lt;/h4&gt;

&lt;p&gt;多probe LSH主要是为了提高查找准确率而引入的一种策略。首先解释一下什么是Multiprobe。对于构建的L个哈希表，我们在每一个哈希表中找到查询样本落入的哈希桶，然后再在这个哈希桶中做遍历，而Multiprobe指的是我们不止在查询样本所在的哈希桶中遍历，还会找到其他的一些哈希桶，然后这些找到的T个哈希桶中进行遍历。这些其他哈希桶的选取准则是：跟查询样本所在的哈希桶邻近的哈希桶，“邻近”指的是汉明距离度量下的邻近。&lt;/p&gt;

&lt;p&gt;通常，如果不使用Multiprobe，我们需要的哈希表数目L在100到1000之间，在处理大数据集的时候，其空间的消耗会非常的高，幸运地是，因为有了上面的Multiprobe的策略，LSH在任意一个哈希表中查找到最近邻的概率变得更高，从而使得我们能到减少哈希表的构建数目。&lt;/p&gt;

&lt;p&gt;综上，对于LSH，涉及到的主要的参数有三个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K，每一个哈希表的哈希函数（空间划分）数目&lt;/li&gt;
  &lt;li&gt;L，哈希表（每一个哈希表有K个哈希函数）的数目&lt;/li&gt;
  &lt;li&gt;T，近邻哈希桶的数目，即the number of probes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这三个设置参数可以按照如下顺序进行：首先，根据可使用的内存大小选取L，然后在K和T之间做出折中：哈希函数数目K越大，相应地，近邻哈希桶的数目的数目T也应该设置得比较大，反之K越小，L也可以相应的减小。获取K和L最优值的方式可以按照如下方式进行：对于每个固定的K，如果在查询样本集上获得了我们想要的精度，则此时T的值即为合理的值。在对T进行调参的时候，我们不需要重新构建哈希表，甚至我们还可以采用二分搜索的方式来加快T参数的选取过程。&lt;/p&gt;

&lt;h3 id=&quot;lsh开源工具包&quot;&gt;LSH开源工具包&lt;/h3&gt;

&lt;p&gt;关于LSH开源工具库，有很多，这里推荐两个LSH开源工具包：&lt;a href=&quot;https://github.com/kayzhu/LSHash&quot;&gt;LSHash&lt;/a&gt;和&lt;a href=&quot;https://falconn-lib.org/&quot;&gt;FALCONN&lt;/a&gt;, 分别对应于学习和应用场景。&lt;/p&gt;

&lt;h4 id=&quot;lshash&quot;&gt;LSHash&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kayzhu/LSHash&quot;&gt;LSHash&lt;/a&gt;非常适合用来学习，里面实现的是最经典的LSH方法，并且还是单表哈希。哈希函数的系数采用随机的方式生成，具体代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_generate_uniform_planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Generate uniformly distributed hyperplanes and return it as a 2D
    numpy array.
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hash_size&lt;/code&gt;为哈希函数的数目，即前面介绍的K。整个框架，不论是LSH的哈希函数的生成方式，还是LSH做查询，都极其的中规中矩，所以用来作为了解LSH的过程，再适合不过。如果要在实用中使用LSH，可以使用&lt;a href=&quot;https://falconn-lib.org/&quot;&gt;FALCONN&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;falconn&quot;&gt;FALCONN&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://falconn-lib.org/&quot;&gt;FALCONN&lt;/a&gt;是经过了极致优化的LSH，其对应的论文为NIPS 2015 &lt;a href=&quot;https://people.csail.mit.edu/ludwigs/papers/nips15_crosspolytopelsh.pdf&quot;&gt;Practical and Optimal LSH for Angular Distance&lt;/a&gt;，&lt;a href=&quot;https://people.csail.mit.edu/indyk/&quot;&gt;Piotr Indyk&lt;/a&gt;系作者之一（Piotr Indyk不知道是谁？&lt;a href=&quot;http://web.mit.edu/andoni/www/LSH/index.html&quot;&gt;E2LSH&lt;/a&gt;这个页面对于看过LSH的应该非常眼熟吧），论文有些晦涩难懂，不过FALCONN工具包却是极其容易使用的，提供有C++使用的例子&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/benchmark/random_benchmark.cc&quot;&gt;random_benchmark.cc&lt;/a&gt;以及Python的例子&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/python/benchmark/random_benchmark.py&quot;&gt;random_benchmark.py&lt;/a&gt;，另外文档非常的详实，具体可参阅&lt;a href=&quot;https://falconn-lib.org/docs/namespacefalconn.html&quot;&gt;falconn Namespace Reference&lt;/a&gt;和&lt;a href=&quot;https://falconn-lib.org/pdoc/falconn/&quot;&gt;falconn module&lt;/a&gt;。下面将其Python例子和C++例子中初始化索引以及构建哈希表的部分提取出来，对其中的参数做一下简要的分析。&lt;/p&gt;

&lt;p&gt;Python初始化与构建索引&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/python/benchmark/random_benchmark.py#L127&quot;&gt;L127&lt;/a&gt;：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Hyperplane hashing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;falconn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHConstructionParameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lsh_family&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'hyperplane'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'negative_inner_product'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;storage_hash_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'flat_hash_table'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_setup_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;833840234&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hyperplane hash&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hp_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;falconn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hp_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hp_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_num_probes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2464&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;C++初始化与构建索引&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/benchmark/random_benchmark.cc#L194&quot;&gt;L194&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Hyperplane hashing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LSHConstructionParameters&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lsh_family&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSHFamily&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hyperplane&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;storage_hash_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storage_hash_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_setup_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_setup_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;833840234&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hyperplane hash&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp_construction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHNearestNeighborTable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hptable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;construct_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hptable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_num_probes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2464&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到，有3个很重要的参数，分别是&lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;l&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;set_num_probes&lt;/code&gt;，对应的具体意义前面已经解释，这里不再赘述。&lt;/p&gt;

&lt;p&gt;FALCONN的索引构建过程非常快，百万量级的数据，维度如果是128维，其构建索引时间大概2-3min的样子，实时搜索可以做到几毫秒的响应时间。总之，这是小白菜见过的构建索引时间最短查询响应时间也极快的ANN工具库。&lt;/p&gt;

&lt;p&gt;另外谈一下数据规模问题。对于小数据集和中型规模的数据集(几个million-几十个million)， FALCONN和&lt;a href=&quot;https://github.com/searchivarius/nmslib&quot;&gt;NMSLIB&lt;/a&gt;是一个非常不错的选择，如果对于大型规模数据集(几百个million以上)，基于矢量量化的&lt;a href=&quot;https://github.com/facebookresearch/faiss/&quot;&gt;Faiss&lt;/a&gt;是一个明智的选择。关于这方面的讨论，可以参阅小白菜参阅的讨论&lt;a href=&quot;https://github.com/facebookresearch/faiss/issues/23&quot;&gt;benchmark&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当然，FALCONN还不是很完善，比如对于数据的动态增删目前还不支持，具体的讨论可以参见&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/issues/2&quot;&gt;Add a dynamic LSH table&lt;/a&gt;。其实这不是FALCONN独有的问题，NMSLIB目前也不支持。一般而言，动态的增删在实际应用场合是一个基本的要求，但是我们应注意到，增删并不是毫无限制的，在增删频繁且持续了一段时间后，这是的数据分布已经不是我们原来建索引的数据分布形式了，我们应该重新构建索引。在这一点上，Faiss支持数据的动态增删。&lt;/p&gt;

&lt;p&gt;对于哈希方法及其典型代表局部敏感哈希，暂时就整理到这里了。下面小白菜对基于矢量量化的方法谈一谈自己理解。&lt;/p&gt;

&lt;h2 id=&quot;矢量量化方法&quot;&gt;矢量量化方法&lt;/h2&gt;

&lt;p&gt;矢量量化方法，即vector quantization，其具体定义为：&lt;a href=&quot;http://blog.pluskid.org/?p=57&quot;&gt;将一个向量空间中的点用其中的一个有限子集来进行编码的过程&lt;/a&gt;。在矢量量化编码中，&lt;a href=&quot;http://baike.baidu.com/item/%E7%9F%A2%E9%87%8F%E9%87%8F%E5%8C%96&quot;&gt;关键是码本的建立和码字搜索算法&lt;/a&gt;。比如常见的聚类算法，就是一种矢量量化方法。而在ANN近似最近邻搜索中，向量量化方法又以乘积量化(PQ, Product Quantization)最为典型。在之前的博文&lt;a href=&quot;http://yongyuan.name/blog/cbir-technique-summary.html&quot;&gt;基于内容的图像检索技术&lt;/a&gt;的最后，对PQ乘积量化的方法做过简单的概要。在这一小节里，小白菜结合自己阅读的论文和代码对PQ乘积量化、倒排乘积量化(IVFPQ)做一种更加直观的解释。&lt;/p&gt;

&lt;h3 id=&quot;pq乘积量化&quot;&gt;PQ乘积量化&lt;/h3&gt;

&lt;p&gt;PQ乘积量化的核心思想还是聚类，或者说具体应用到ANN近似最近邻搜索上，K-Means是PQ乘积量化子空间数目为1的特例。PQ乘积量化生成码本和量化的过程可以用如下图示来说明：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/PQ_zpsybhprown.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在训练阶段，针对N个训练样本，假设样本维度为128维，我们将其切分为4个子空间，则每一个子空间的维度为32维，然后我们在每一个子空间中，对子向量采用K-Means对其进行聚类(图中示意聚成256类)，这样每一个子空间都能得到一个码本。这样训练样本的每个子段，都可以用子空间的聚类中心来近似，对应的编码即为类中心的ID。如图所示，通过这样一种编码方式，训练样本仅使用的很短的一个编码得以表示，从而达到量化的目的。对于待编码的样本，将它进行相同的切分，然后在各个子空间里逐一找到距离它们最近的类中心，然后用类中心的id来表示它们，即完成了待编码样本的编码。&lt;/p&gt;

&lt;p&gt;正如前面所说的，在矢量量化编码中，关键是码本的建立和码字的搜索算法，在上面，我们得到了建立的码本以及量化编码的方式。剩下的重点就是查询样本与dataset中的样本距离如何计算的问题了。&lt;/p&gt;

&lt;p&gt;在查询阶段，PQ同样在计算查询样本与dataset中各个样本的距离，只不过这种距离的计算转化为间接近似的方法而获得。PQ乘积量化方法在计算距离的时候，有两种距离计算方式，一种是对称距离，另外一种是非对称距离。非对称距离的损失小(也就是更接近真实距离)，实际中也经常采用这种距离计算方式。下面过程示意的是查询样本来到时，以非对称距离的方式(红框标识出来的部分)计算到dataset样本间的计算示意：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/PQ_search_zpskgugtocx.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体地，查询向量来到时，按训练样本生成码本的过程，将其同样分成相同的子段，然后在每个子空间中，计算子段到该子空间中所有聚类中心得距离，如图中所示，可以得到4*256个距离，这里为便于后面的理解说明，小白菜就把这些算好的距离称作距离池。在计算库中某个样本到查询向量的距离时，比如编码为(124, 56, 132, 222)这个样本到查询向量的距离时，我们分别到距离池中取各个子段对应的距离即可，比如编码为124这个子段，在第1个算出的256个距离里面把编号为124的那个距离取出来就可，所有子段对应的距离取出来后，将这些子段的距离求和相加，即得到该样本到查询样本间的非对称距离。所有距离算好后，排序后即得到我们最终想要的结果。&lt;/p&gt;

&lt;p&gt;从上面这个过程可以很清楚地看出PQ乘积量化能够加速索引的原理：即将全样本的距离计算，转化为到子空间类中心的距离计算。比如上面所举的例子，原本brute-force search的方式计算距离的次数随样本数目N成线性增长，但是经过PQ编码后，对于耗时的距离计算，只要计算4*256次，几乎可以忽略此时间的消耗。另外，从上图也可以看出，对特征进行编码后，可以用一个相对比较短的编码来表示样本，自然对于内存的消耗要大大小于brute-force search的方式。&lt;/p&gt;

&lt;p&gt;在某些特殊的场合，我们总是希望获得精确的距离，而不是近似的距离，并且我们总是喜欢获取向量间的余弦相似度（余弦相似度距离范围在[-1,1]之间，便于设置固定的阈值），针对这种场景，可以针对PQ乘积量化得到的前top@K做一个brute-force search的排序。&lt;/p&gt;

&lt;h3 id=&quot;倒排乘积量化&quot;&gt;倒排乘积量化&lt;/h3&gt;

&lt;p&gt;倒排PQ乘积量化(IVFPQ)是PQ乘积量化的更进一步加速版。其加速的本质逃不开小白菜在最前面强调的是加速原理：&lt;strong&gt;brute-force搜索的方式是在全空间进行搜索，为了加快查找的速度，几乎所有的ANN方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历&lt;/strong&gt;。在上一小节可以看出，PQ乘积量化计算距离的时候，距离虽然已经预先算好了，但是对于每个样本到查询样本的距离，还是得老老实实挨个去求和相加计算距离。但是，实际上我们感兴趣的是那些跟查询样本相近的样本（小白菜称这样的区域为感兴趣区域），也就是说老老实实挨个相加其实做了很多的无用功，如果能够通过某种手段快速将全局遍历锁定为感兴趣区域，则可以舍去不必要的全局计算以及排序。倒排PQ乘积量化的”倒排“，正是这样一种思想的体现，在具体实施手段上，采用的是通过聚类的方式实现感兴趣区域的快速定位，在倒排PQ乘积量化中，聚类可以说应用得淋漓尽致。&lt;/p&gt;

&lt;p&gt;倒排PQ乘积量化整个过程如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/IVFPQ_zpswvosr8a6.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在PQ乘积量化之前，增加了一个粗量化过程。具体地，先对N个训练样本采用K-Means进行聚类，这里聚类的数目一般设置得不应过大，一般设置为1024差不多，这种可以以比较快的速度完成聚类过程。得到了聚类中心后，针对每一个样本x_i，找到其距离最近的类中心c_i后，两者相减得到样本x_i的残差向量(x_i-c_i)，后面剩下的过程，就是针对(x_i-c_i)的PQ乘积量化过程，此过程不再赘述。&lt;/p&gt;

&lt;p&gt;在查询的时候，通过相同的粗量化，可以快速定位到查询向量属于哪个c_i（即在哪一个感兴趣区域），然后在该感兴趣区域按上面所述的PQ乘积量化距离计算方式计算距离。&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Apr 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:5001//blog/ann-search.html</link>
        <guid isPermaLink="true">http://localhost:5001//blog/ann-search.html</guid>
      </item>
    
  </channel>
</rss>
